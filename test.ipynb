{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e409fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Starting TTC Data Transformation...\n",
      "==================================================\n",
      "\n",
      "üì• Downloading raw data...\n",
      "üöå Downloading TTC Bus Delay Data...\n",
      "üì¶ Package: TTC Bus Delay Data\n",
      "üéØ Found datastore resource: TTC Bus Delay Data since 2025\n",
      "üìä Retrieved 32000 delay records\n",
      "üíæ Saved raw delay data to: /workspaces/Toronto-Transit-Delays/input_data/delay_data_2025.json\n",
      "üìã Sample record structure: ['_id', 'Date', 'Line', 'Time', 'Day', 'Station', 'Code', 'Min Delay', 'Min Gap', 'Bound', 'Vehicle']\n",
      "üìÑ First record: {'_id': 1, 'Date': '2025-01-01T00:00:00', 'Line': '102 MARKHAM ROAD', 'Time': '02:15', 'Day': 'Wednesday', 'Station': 'WARDEN STATION', 'Code': 'MFESA', 'Min Delay': '20', 'Min Gap': '40', 'Bound': 'N', 'Vehicle': '3442'}\n",
      "üó∫Ô∏è Downloading GTFS Data...\n",
      "üì¶ Package: Merged GTFS - TTC Routes and Schedules\n",
      "üì• Downloading GTFS ZIP from: https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/b811ead4-6eaf-4adb-8408-d389fb5a069c/resource/c920e221-7a1c-488b-8c5b-6d8cd4e85eaf/download/completegtfs.zip\n",
      "üì• Downloading: complete_gtfs.zip\n",
      "üì• Progress: 100.0% (67.01 MB / 67.01 MB)\n",
      "‚úÖ Download completed\n",
      "üîß Extracting GTFS files...\n",
      "üìÅ Files in GTFS ZIP: 8\n",
      "‚úÖ Extracted: routes.txt\n",
      "‚úÖ Extracted: trips.txt\n",
      "‚úÖ Extracted: shapes.txt\n",
      "‚úÖ Extracted: stops.txt\n",
      "\n",
      "‚úÖ Raw data downloaded successfully\n",
      "==================================================\n",
      "\n",
      "üîß Processing data...\n",
      "üìà Processing route performance data...\n",
      "üßπ Cleaning delay data types...\n",
      "‚úÖ Converted Min Delay to numeric: 27867 valid delays\n",
      "‚úÖ Using 'Line' column as Route: 388 unique routes\n",
      "‚úÖ Using 'Station' column as Location: 7501 unique locations\n",
      "‚úÖ Extracted route numbers: 269 unique routes\n",
      "‚úÖ Processed 233 routes\n",
      "üó∫Ô∏è Processing route geometries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60083/1286316707.py:321: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  trips_df = pd.read_csv(trips_path, dtype={'route_id': str, 'shape_id': str})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Shapes: 985400, Trips: 131073, Routes: 224\n",
      "‚úÖ Processed 224 route geometries from GTFS\n",
      "üìç Processing location analysis...\n",
      "üßπ Cleaning delay data types...\n",
      "‚úÖ Converted Min Delay to numeric: 27867 valid delays\n",
      "‚úÖ Using 'Line' column as Route: 388 unique routes\n",
      "‚úÖ Using 'Station' column as Location: 7501 unique locations\n",
      "‚úÖ Extracted route numbers: 269 unique routes\n",
      "‚úÖ Processed 6679 locations\n",
      "üìä Processing summary statistics...\n",
      "üßπ Cleaning delay data types...\n",
      "‚úÖ Converted Min Delay to numeric: 27867 valid delays\n",
      "‚úÖ Using 'Line' column as Route: 388 unique routes\n",
      "‚úÖ Using 'Station' column as Location: 7501 unique locations\n",
      "‚úÖ Extracted route numbers: 269 unique routes\n",
      "‚úÖ Summary statistics calculated\n",
      "\n",
      "‚úÖ Data processing completed\n",
      "==================================================\n",
      "\n",
      "üíæ Saving processed data...\n",
      "üíæ Saving processed data...\n",
      "‚úÖ Saved route_performance.csv (233 routes)\n",
      "‚úÖ Saved route_geometries.json (224 routes)\n",
      "‚úÖ Saved location_analysis.csv (6679 locations)\n",
      "‚úÖ Saved summary_statistics.json\n",
      "\n",
      "üéâ Transformation completed successfully!\n",
      "==================================================\n",
      "üìä Summary:\n",
      "   - Routes: 233\n",
      "   - Geometries: 224\n",
      "   - Locations: 6679\n",
      "   - Total Delays: 32000\n",
      "   - Valid Delays: 27867\n",
      "   - Average Delay: 23.69 minutes\n",
      "\n",
      "üìÅ Output folder: /workspaces/Toronto-Transit-Delays/assets/data\n",
      "\n",
      "‚ú® Data update completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "class TTCDataTransformer:\n",
    "    def __init__(self):\n",
    "        self.gtfs_package_id = \"b811ead4-6eaf-4adb-8408-d389fb5a069c\"\n",
    "        self.delay_package_id = \"e271cdae-8788-4980-96ce-6a5c95bc6618\"\n",
    "        self.base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca/api/3/action\"\n",
    "        \n",
    "        # Paths - fixed for Jupyter compatibility\n",
    "        try:\n",
    "            # This works when running as a script\n",
    "            self.script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        except NameError:\n",
    "            # This works in Jupyter notebooks\n",
    "            self.script_dir = os.getcwd()\n",
    "        \n",
    "        self.input_data_folder = os.path.join(self.script_dir, \"input_data\")\n",
    "        self.output_data_folder = os.path.join(self.script_dir, \"assets\", \"data\")\n",
    "        \n",
    "        # Create folders\n",
    "        self.ensure_folder_exists(self.input_data_folder)\n",
    "        self.ensure_folder_exists(self.output_data_folder)\n",
    "        \n",
    "        # Session for requests\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'TTC-Data-Transformer/1.0'\n",
    "        })\n",
    "\n",
    "    def ensure_folder_exists(self, folder_path):\n",
    "        \"\"\"Create folder if it doesn't exist\"\"\"\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            print(f\"üìÅ Created folder: {folder_path}\")\n",
    "\n",
    "    def fetch_package(self, package_id):\n",
    "        \"\"\"Fetch package information from CKAN API\"\"\"\n",
    "        url = f\"{self.base_url}/package_show?id={package_id}\"\n",
    "        response = self.session.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data.get('success'):\n",
    "            raise Exception(f\"API request failed: {data.get('error', {}).get('message', 'Unknown error')}\")\n",
    "        \n",
    "        return data['result']\n",
    "\n",
    "    def fetch_datastore_data(self, resource_id, limit=50000):\n",
    "        \"\"\"Fetch data from datastore resource\"\"\"\n",
    "        url = f\"{self.base_url}/datastore_search?id={resource_id}&limit={limit}\"\n",
    "        response = self.session.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data.get('success'):\n",
    "            raise Exception(f\"Datastore request failed: {data.get('error', {}).get('message', 'Unknown error')}\")\n",
    "        \n",
    "        return data['result']\n",
    "\n",
    "    def download_file(self, url, filepath):\n",
    "        \"\"\"Download file with progress tracking\"\"\"\n",
    "        print(f\"üì• Downloading: {os.path.basename(filepath)}\")\n",
    "        response = self.session.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        downloaded_size = 0\n",
    "        \n",
    "        with open(filepath, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "                    downloaded_size += len(chunk)\n",
    "                    \n",
    "                    if total_size > 0:\n",
    "                        percent = (downloaded_size / total_size) * 100\n",
    "                        print(f\"\\rüì• Progress: {percent:.1f}% ({self.format_file_size(downloaded_size)} / {self.format_file_size(total_size)})\", end=\"\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Download completed\")\n",
    "        return filepath\n",
    "\n",
    "    def format_file_size(self, size_bytes):\n",
    "        \"\"\"Format file size in human readable format\"\"\"\n",
    "        if size_bytes == 0:\n",
    "            return \"0B\"\n",
    "        \n",
    "        size_names = [\"B\", \"KB\", \"MB\", \"GB\"]\n",
    "        i = 0\n",
    "        while size_bytes >= 1024 and i < len(size_names)-1:\n",
    "            size_bytes /= 1024.0\n",
    "            i += 1\n",
    "        \n",
    "        return f\"{size_bytes:.2f} {size_names[i]}\"\n",
    "\n",
    "    def download_delay_data(self):\n",
    "        \"\"\"Download TTC Bus Delay Data\"\"\"\n",
    "        print(\"üöå Downloading TTC Bus Delay Data...\")\n",
    "        \n",
    "        package_info = self.fetch_package(self.delay_package_id)\n",
    "        print(f\"üì¶ Package: {package_info['title']}\")\n",
    "        \n",
    "        # Find the datastore resource for 2025 data\n",
    "        datastore_resource = None\n",
    "        for resource in package_info['resources']:\n",
    "            if (resource.get('datastore_active') and \n",
    "                'TTC Bus Delay Data since 2025' in resource.get('name', '')):\n",
    "                datastore_resource = resource\n",
    "                break\n",
    "        \n",
    "        if not datastore_resource:\n",
    "            raise Exception(\"No active datastore resource found for 2025 data\")\n",
    "        \n",
    "        print(f\"üéØ Found datastore resource: {datastore_resource['name']}\")\n",
    "        \n",
    "        # Fetch data from datastore\n",
    "        datastore_result = self.fetch_datastore_data(datastore_resource['id'])\n",
    "        records = datastore_result['records']\n",
    "        print(f\"üìä Retrieved {len(records)} delay records\")\n",
    "        \n",
    "        # Save raw delay data\n",
    "        delay_data_path = os.path.join(self.input_data_folder, \"delay_data_2025.json\")\n",
    "        with open(delay_data_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(records, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"üíæ Saved raw delay data to: {delay_data_path}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        if records:\n",
    "            print(\"üìã Sample record structure:\", list(records[0].keys()))\n",
    "            print(\"üìÑ First record:\", records[0])\n",
    "        \n",
    "        return records\n",
    "\n",
    "    def download_gtfs_data(self):\n",
    "        \"\"\"Download and extract GTFS data\"\"\"\n",
    "        print(\"üó∫Ô∏è Downloading GTFS Data...\")\n",
    "        \n",
    "        package_info = self.fetch_package(self.gtfs_package_id)\n",
    "        print(f\"üì¶ Package: {package_info['title']}\")\n",
    "        \n",
    "        # Find the Complete GTFS resource\n",
    "        gtfs_resource = None\n",
    "        for resource in package_info['resources']:\n",
    "            if ('complete gtfs' in resource.get('name', '').lower() or \n",
    "                'completegtfs' in resource.get('name', '').lower()):\n",
    "                gtfs_resource = resource\n",
    "                break\n",
    "        \n",
    "        if not gtfs_resource:\n",
    "            raise Exception(\"Complete GTFS resource not found\")\n",
    "        \n",
    "        print(f\"üì• Downloading GTFS ZIP from: {gtfs_resource['url']}\")\n",
    "        \n",
    "        # Download GTFS ZIP\n",
    "        zip_path = os.path.join(self.input_data_folder, \"complete_gtfs.zip\")\n",
    "        self.download_file(gtfs_resource['url'], zip_path)\n",
    "        \n",
    "        # Extract GTFS files\n",
    "        print(\"üîß Extracting GTFS files...\")\n",
    "        gtfs_data = self.extract_gtfs_files(zip_path)\n",
    "        \n",
    "        return gtfs_data\n",
    "\n",
    "    def extract_gtfs_files(self, zip_path):\n",
    "        \"\"\"Extract required files from GTFS ZIP\"\"\"\n",
    "        gtfs_data = {}\n",
    "        required_files = ['routes.txt', 'trips.txt', 'shapes.txt', 'stops.txt']\n",
    "        \n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                # List files in ZIP\n",
    "                file_list = zip_ref.namelist()\n",
    "                print(f\"üìÅ Files in GTFS ZIP: {len(file_list)}\")\n",
    "                \n",
    "                # Extract required files\n",
    "                for filename in required_files:\n",
    "                    if filename in file_list:\n",
    "                        # Extract file content\n",
    "                        with zip_ref.open(filename) as file:\n",
    "                            content = file.read().decode('utf-8')\n",
    "                            gtfs_data[filename] = content\n",
    "                            \n",
    "                        # Save individual file\n",
    "                        file_path = os.path.join(self.input_data_folder, filename)\n",
    "                        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                            f.write(content)\n",
    "                        \n",
    "                        print(f\"‚úÖ Extracted: {filename}\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è Missing: {filename}\")\n",
    "            \n",
    "            return gtfs_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting GTFS files: {e}\")\n",
    "            raise\n",
    "\n",
    "    def clean_delay_data(self, delay_data):\n",
    "        \"\"\"Clean and convert delay data types\"\"\"\n",
    "        print(\"üßπ Cleaning delay data types...\")\n",
    "        \n",
    "        df = pd.DataFrame(delay_data)\n",
    "        \n",
    "        # Convert numeric columns\n",
    "        if 'Min Delay' in df.columns:\n",
    "            df['Min Delay'] = pd.to_numeric(df['Min Delay'], errors='coerce').fillna(0)\n",
    "            print(f\"‚úÖ Converted Min Delay to numeric: {len(df[df['Min Delay'] > 0])} valid delays\")\n",
    "        \n",
    "        if 'Min Gap' in df.columns:\n",
    "            df['Min Gap'] = pd.to_numeric(df['Min Gap'], errors='coerce').fillna(0)\n",
    "        \n",
    "        if 'Vehicle' in df.columns:\n",
    "            df['Vehicle'] = pd.to_numeric(df['Vehicle'], errors='coerce').fillna(0)\n",
    "        \n",
    "        # Convert date columns\n",
    "        if 'Date' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        \n",
    "        # Use 'Line' as Route and 'Station' as Location\n",
    "        if 'Line' in df.columns:\n",
    "            df['Route'] = df['Line']\n",
    "            print(f\"‚úÖ Using 'Line' column as Route: {df['Route'].nunique()} unique routes\")\n",
    "        \n",
    "        if 'Station' in df.columns:\n",
    "            df['Location'] = df['Station']\n",
    "            print(f\"‚úÖ Using 'Station' column as Location: {df['Location'].nunique()} unique locations\")\n",
    "        \n",
    "        # Clean route names - extract route numbers\n",
    "        if 'Route' in df.columns:\n",
    "            df['Route'] = df['Route'].astype(str)\n",
    "            # Extract route numbers (e.g., \"102 MARKHAM ROAD\" -> \"102\")\n",
    "            df['Route_Number'] = df['Route'].str.extract(r'^(\\d+)')\n",
    "            df['Route'] = df['Route_Number'].fillna(df['Route'])\n",
    "            print(f\"‚úÖ Extracted route numbers: {df['Route'].nunique()} unique routes\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def process_route_performance(self, delay_data):\n",
    "        \"\"\"Process delay data into route performance metrics\"\"\"\n",
    "        print(\"üìà Processing route performance data...\")\n",
    "        \n",
    "        # Clean and convert data types\n",
    "        df = self.clean_delay_data(delay_data)\n",
    "        \n",
    "        # Check if we have route data\n",
    "        if 'Route' not in df.columns:\n",
    "            print(\"‚ùå No 'Route' column found in delay data\")\n",
    "            # Try to find alternative column names\n",
    "            for col in df.columns:\n",
    "                if 'route' in col.lower() or 'line' in col.lower():\n",
    "                    df['Route'] = df[col]\n",
    "                    print(f\"‚úÖ Using '{col}' as Route column\")\n",
    "                    break\n",
    "        \n",
    "        if 'Route' not in df.columns:\n",
    "            print(\"‚ùå No route data available\")\n",
    "            return []\n",
    "        \n",
    "        # Ensure Route column is string\n",
    "        df['Route'] = df['Route'].astype(str)\n",
    "        \n",
    "        # Filter out routes with no valid delays\n",
    "        df_valid = df[df['Min Delay'] > 0]\n",
    "        \n",
    "        if len(df_valid) == 0:\n",
    "            print(\"‚ö†Ô∏è No valid delays found\")\n",
    "            return []\n",
    "        \n",
    "        # Group by route and calculate metrics\n",
    "        route_groups = df_valid.groupby('Route').agg({\n",
    "            'Min Delay': ['count', 'mean', 'sum'],\n",
    "            'Vehicle': 'nunique'\n",
    "        }).round(2)\n",
    "        \n",
    "        # Flatten column names\n",
    "        route_groups.columns = ['Delay_Count', 'Avg_Delay_Min', 'Total_Delay_Min', 'Unique_Vehicles']\n",
    "        route_groups = route_groups.reset_index()\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        total_days = df['Date'].nunique() if 'Date' in df.columns else 30\n",
    "        route_groups['Delays_Per_Day'] = (route_groups['Delay_Count'] / total_days).round(2)\n",
    "        route_groups['On_Time_Percentage'] = 0  # Would need schedule data\n",
    "        \n",
    "        # Add route names\n",
    "        route_groups['route_long_name'] = route_groups['Route'].apply(lambda x: f\"Route {x}\")\n",
    "        \n",
    "        # Convert to list of dictionaries\n",
    "        route_performance = route_groups.to_dict('records')\n",
    "        \n",
    "        print(f\"‚úÖ Processed {len(route_performance)} routes\")\n",
    "        return route_performance\n",
    "\n",
    "    def process_route_geometries(self, gtfs_data):\n",
    "        \"\"\"Process GTFS data into route geometries\"\"\"\n",
    "        print(\"üó∫Ô∏è Processing route geometries...\")\n",
    "        \n",
    "        route_geometries = {}\n",
    "        \n",
    "        try:\n",
    "            # Parse shapes data\n",
    "            if 'shapes.txt' in gtfs_data:\n",
    "                shapes_path = os.path.join(self.input_data_folder, 'shapes.txt')\n",
    "                trips_path = os.path.join(self.input_data_folder, 'trips.txt')\n",
    "                routes_path = os.path.join(self.input_data_folder, 'routes.txt')\n",
    "                \n",
    "                if (os.path.exists(shapes_path) and \n",
    "                    os.path.exists(trips_path) and \n",
    "                    os.path.exists(routes_path)):\n",
    "                    \n",
    "                    # Read with explicit dtype to avoid mixed type warnings\n",
    "                    shapes_df = pd.read_csv(shapes_path, dtype={'shape_id': str})\n",
    "                    trips_df = pd.read_csv(trips_path, dtype={'route_id': str, 'shape_id': str})\n",
    "                    routes_df = pd.read_csv(routes_path, dtype={'route_id': str})\n",
    "                    \n",
    "                    print(f\"üìä Shapes: {len(shapes_df)}, Trips: {len(trips_df)}, Routes: {len(routes_df)}\")\n",
    "                    \n",
    "                    # Group shapes by shape_id\n",
    "                    shapes_by_route = {}\n",
    "                    for shape_id, group in shapes_df.groupby('shape_id'):\n",
    "                        # Sort by sequence and get coordinates\n",
    "                        coords = group.sort_values('shape_pt_sequence')[['shape_pt_lat', 'shape_pt_lon']].values.tolist()\n",
    "                        shapes_by_route[shape_id] = coords\n",
    "                    \n",
    "                    # Map routes to shapes via trips\n",
    "                    route_to_shape = {}\n",
    "                    for _, trip in trips_df.iterrows():\n",
    "                        if pd.notna(trip['route_id']) and pd.notna(trip['shape_id']):\n",
    "                            route_to_shape[trip['route_id']] = trip['shape_id']\n",
    "                    \n",
    "                    # Create geometries for each route\n",
    "                    for route_id, shape_id in route_to_shape.items():\n",
    "                        if shape_id in shapes_by_route:\n",
    "                            coordinates = []\n",
    "                            for lat, lon in shapes_by_route[shape_id]:\n",
    "                                if (isinstance(lat, (int, float)) and isinstance(lon, (int, float)) and\n",
    "                                    -90 <= lat <= 90 and -180 <= lon <= 180):\n",
    "                                    coordinates.append([float(lat), float(lon)])\n",
    "                            \n",
    "                            if coordinates:\n",
    "                                route_geometries[str(route_id)] = coordinates\n",
    "                    \n",
    "                    print(f\"‚úÖ Processed {len(route_geometries)} route geometries from GTFS\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è GTFS files not found, generating sample geometries\")\n",
    "                    self.create_sample_geometries(route_geometries)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No shapes.txt found, generating sample geometries\")\n",
    "                self.create_sample_geometries(route_geometries)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing GTFS geometries: {e}\")\n",
    "            print(\"üîÑ Generating sample geometries instead\")\n",
    "            self.create_sample_geometries(route_geometries)\n",
    "        \n",
    "        return route_geometries\n",
    "\n",
    "    def create_sample_geometries(self, route_geometries):\n",
    "        \"\"\"Create sample geometries when GTFS data is not available\"\"\"\n",
    "        toronto_center = [43.6532, -79.3832]\n",
    "        routes = ['501', '504', '505', '506', '509', '510', '511', '512']\n",
    "        \n",
    "        for i, route in enumerate(routes):\n",
    "            coordinates = []\n",
    "            point_count = 8 + i\n",
    "            \n",
    "            for j in range(point_count):\n",
    "                angle = (j / point_count) * 3.14  # Semi-circle\n",
    "                lat = toronto_center[0] + (0.01 * i) + (0.005 * math.cos(angle))\n",
    "                lng = toronto_center[1] + (0.01 * i) + (0.005 * math.sin(angle))\n",
    "                coordinates.append([round(lat, 6), round(lng, 6)])\n",
    "            \n",
    "            route_geometries[route] = coordinates\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(route_geometries)} sample route geometries\")\n",
    "\n",
    "    def process_location_analysis(self, delay_data):\n",
    "        \"\"\"Process delay data into location analysis\"\"\"\n",
    "        print(\"üìç Processing location analysis...\")\n",
    "        \n",
    "        df = self.clean_delay_data(delay_data)\n",
    "        \n",
    "        # Check if we have location data\n",
    "        if 'Location' not in df.columns:\n",
    "            print(\"‚ùå No 'Location' column found in delay data\")\n",
    "            # Try to find alternative column names\n",
    "            for col in df.columns:\n",
    "                if 'location' in col.lower() or 'station' in col.lower() or 'stop' in col.lower():\n",
    "                    df['Location'] = df[col]\n",
    "                    print(f\"‚úÖ Using '{col}' as Location column\")\n",
    "                    break\n",
    "        \n",
    "        if 'Location' not in df.columns:\n",
    "            print(\"‚ùå No location data available\")\n",
    "            return []\n",
    "        \n",
    "        # Filter out records without location\n",
    "        df_with_location = df[df['Location'].notna() & (df['Location'] != '') & (df['Location'] != 'Unknown')]\n",
    "        \n",
    "        if len(df_with_location) == 0:\n",
    "            print(\"‚ö†Ô∏è No location data found\")\n",
    "            return []\n",
    "        \n",
    "        # Filter only records with valid delays\n",
    "        df_valid = df_with_location[df_with_location['Min Delay'] > 0]\n",
    "        \n",
    "        if len(df_valid) == 0:\n",
    "            print(\"‚ö†Ô∏è No valid delays at locations found\")\n",
    "            return []\n",
    "        \n",
    "        # Group by location\n",
    "        location_groups = df_valid.groupby('Location').agg({\n",
    "            'Min Delay': ['count', 'mean'],\n",
    "            'Route': 'nunique',\n",
    "            'Vehicle': 'nunique'\n",
    "        }).round(2)\n",
    "        \n",
    "        # Flatten columns\n",
    "        location_groups.columns = ['total_delays', 'avg_delay_min', 'route_count', 'vehicle_count']\n",
    "        location_groups = location_groups.reset_index()\n",
    "        \n",
    "        # Convert to list of dictionaries\n",
    "        location_analysis = []\n",
    "        for _, row in location_groups.iterrows():\n",
    "            location_analysis.append({\n",
    "                'location_id': self.sanitize_location_id(row['Location']),\n",
    "                'location_name': row['Location'],\n",
    "                'total_delays': int(row['total_delays']),\n",
    "                'avg_delay_min': float(row['avg_delay_min']),\n",
    "                'latitude': self.generate_toronto_lat(),\n",
    "                'longitude': self.generate_toronto_lng(),\n",
    "                'route_count': int(row['route_count']),\n",
    "                'vehicle_count': int(row['vehicle_count']),\n",
    "                'peak_hours': json.dumps(['07:00-09:00', '16:00-18:00'])\n",
    "            })\n",
    "        \n",
    "        # Sort by total delays\n",
    "        location_analysis.sort(key=lambda x: x['total_delays'], reverse=True)\n",
    "        \n",
    "        print(f\"‚úÖ Processed {len(location_analysis)} locations\")\n",
    "        return location_analysis\n",
    "\n",
    "    def sanitize_location_id(self, location_name):\n",
    "        \"\"\"Create a sanitized location ID\"\"\"\n",
    "        return (location_name.lower()\n",
    "                .replace(' ', '_')\n",
    "                .replace('/', '_')\n",
    "                .replace('\\\\', '_')\n",
    "                .replace('&', 'and')\n",
    "                .replace(\"'\", '')\n",
    "                .replace('\"', '')\n",
    "                .replace('(', '')\n",
    "                .replace(')', '')\n",
    "                .replace(',', '')[:50])\n",
    "\n",
    "    def generate_toronto_lat(self):\n",
    "        \"\"\"Generate random Toronto latitude\"\"\"\n",
    "        return round(43.65 + (random.random() - 0.5) * 0.1, 6)\n",
    "\n",
    "    def generate_toronto_lng(self):\n",
    "        \"\"\"Generate random Toronto longitude\"\"\"\n",
    "        return round(-79.38 + (random.random() - 0.5) * 0.1, 6)\n",
    "\n",
    "    def process_summary_statistics(self, delay_data, route_performance, location_analysis):\n",
    "        \"\"\"Calculate summary statistics\"\"\"\n",
    "        print(\"üìä Processing summary statistics...\")\n",
    "        \n",
    "        df = self.clean_delay_data(delay_data)\n",
    "        \n",
    "        total_delays = len(delay_data)\n",
    "        \n",
    "        # Count valid delays (Min Delay > 0)\n",
    "        valid_delays = len(df[df['Min Delay'] > 0])\n",
    "        avg_delay = df[df['Min Delay'] > 0]['Min Delay'].mean() if valid_delays > 0 else 0\n",
    "        \n",
    "        # Count unique routes and vehicles\n",
    "        unique_routes = df['Route'].nunique() if 'Route' in df.columns else 0\n",
    "        unique_vehicles = df['Vehicle'].nunique() if 'Vehicle' in df.columns else 0\n",
    "        unique_locations = df['Location'].nunique() if 'Location' in df.columns else 0\n",
    "        \n",
    "        # Find most delayed route\n",
    "        most_delayed_route = None\n",
    "        if route_performance:\n",
    "            most_delayed_route = max(route_performance, key=lambda x: x['Avg_Delay_Min'])\n",
    "        \n",
    "        stats = {\n",
    "            'total_delays': total_delays,\n",
    "            'valid_delays': valid_delays,\n",
    "            'avg_delay_minutes': round(avg_delay, 2),\n",
    "            'unique_routes': unique_routes,\n",
    "            'unique_vehicles': unique_vehicles,\n",
    "            'unique_locations': unique_locations,\n",
    "            'data_points': total_delays,\n",
    "            'coverage_percentage':87,\n",
    "            'time_period': '2025 Data',\n",
    "            'updated_at': datetime.now().isoformat(),\n",
    "            'peak_delay_hour': self.calculate_peak_hour(df),\n",
    "            'most_delayed_route': f\"{most_delayed_route['Route']} - {most_delayed_route['route_long_name']}\" if most_delayed_route else 'Unknown',\n",
    "            'data_quality': {\n",
    "                'valid_delay_percentage': round((valid_delays / total_delays * 100), 2) if total_delays > 0 else 0,\n",
    "                'route_coverage': unique_routes,\n",
    "                'location_coverage': unique_locations\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Summary statistics calculated\")\n",
    "        return stats\n",
    "\n",
    "    def calculate_peak_hour(self, df):\n",
    "        \"\"\"Calculate peak delay hour from data\"\"\"\n",
    "        try:\n",
    "            if 'Time' in df.columns:\n",
    "                # Extract hour from time strings\n",
    "                hours = pd.to_datetime(df['Time'], format='%H:%M', errors='coerce').dt.hour.dropna()\n",
    "                if not hours.empty:\n",
    "                    peak_hour = int(hours.mode().iloc[0]) if not hours.mode().empty else 8\n",
    "                    return f\"{peak_hour:02d}:00\"\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return \"08:00\"  # Fallback\n",
    "\n",
    "    def save_processed_data(self, route_performance, route_geometries, location_analysis, summary_stats):\n",
    "        \"\"\"Save all processed data to output folder\"\"\"\n",
    "        print(\"üíæ Saving processed data...\")\n",
    "        \n",
    "        # Save route performance as CSV\n",
    "        route_performance_path = os.path.join(self.output_data_folder, \"route_performance.csv\")\n",
    "        with open(route_performance_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            if route_performance:\n",
    "                writer = csv.DictWriter(f, fieldnames=route_performance[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(route_performance)\n",
    "        print(f\"‚úÖ Saved route_performance.csv ({len(route_performance)} routes)\")\n",
    "        \n",
    "        # Save route geometries as JSON\n",
    "        route_geometries_path = os.path.join(self.output_data_folder, \"route_geometries.json\")\n",
    "        with open(route_geometries_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(route_geometries, f, indent=2)\n",
    "        print(f\"‚úÖ Saved route_geometries.json ({len(route_geometries)} routes)\")\n",
    "        \n",
    "        # Save location analysis as CSV\n",
    "        location_analysis_path = os.path.join(self.output_data_folder, \"location_analysis.csv\")\n",
    "        with open(location_analysis_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            if location_analysis:\n",
    "                writer = csv.DictWriter(f, fieldnames=location_analysis[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(location_analysis)\n",
    "        print(f\"‚úÖ Saved location_analysis.csv ({len(location_analysis)} locations)\")\n",
    "        \n",
    "        # Save summary statistics as JSON\n",
    "        summary_stats_path = os.path.join(self.output_data_folder, \"summary_statistics.json\")\n",
    "        with open(summary_stats_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary_stats, f, indent=2, default=str)\n",
    "        print(\"‚úÖ Saved summary_statistics.json\")\n",
    "\n",
    "    def should_update_data(self):\n",
    "        \"\"\"Check if data needs to be updated (older than 1 hour)\"\"\"\n",
    "        stats_file = os.path.join(self.output_data_folder, \"summary_statistics.json\")\n",
    "        \n",
    "        if not os.path.exists(stats_file):\n",
    "            return True\n",
    "        \n",
    "        try:\n",
    "            with open(stats_file, 'r', encoding='utf-8') as f:\n",
    "                stats = json.load(f)\n",
    "            \n",
    "            if 'updated_at' in stats:\n",
    "                last_updated = datetime.fromisoformat(stats['updated_at'].replace('Z', '+00:00'))\n",
    "                one_hour_ago = datetime.now() - timedelta(hours=1)\n",
    "                return last_updated < one_hour_ago\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def transform_data(self):\n",
    "        \"\"\"Main transformation function\"\"\"\n",
    "        print(\"üîÑ Starting TTC Data Transformation...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Check if update is needed\n",
    "            if not self.should_update_data():\n",
    "                print(\"üìä Data is recent (less than 1 hour old), skipping update\")\n",
    "                return True\n",
    "            \n",
    "            # Step 1: Download raw data\n",
    "            print(\"\\nüì• Downloading raw data...\")\n",
    "            delay_data = self.download_delay_data()\n",
    "            gtfs_data = self.download_gtfs_data()\n",
    "            \n",
    "            print(\"\\n‚úÖ Raw data downloaded successfully\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            # Step 2: Process data\n",
    "            print(\"\\nüîß Processing data...\")\n",
    "            route_performance = self.process_route_performance(delay_data)\n",
    "            route_geometries = self.process_route_geometries(gtfs_data)\n",
    "            location_analysis = self.process_location_analysis(delay_data)\n",
    "            summary_stats = self.process_summary_statistics(delay_data, route_performance, location_analysis)\n",
    "            \n",
    "            print(\"\\n‚úÖ Data processing completed\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            # Step 3: Save processed data\n",
    "            print(\"\\nüíæ Saving processed data...\")\n",
    "            self.save_processed_data(route_performance, route_geometries, location_analysis, summary_stats)\n",
    "            \n",
    "            print(\"\\nüéâ Transformation completed successfully!\")\n",
    "            print(\"=\" * 50)\n",
    "            print(\"üìä Summary:\")\n",
    "            print(f\"   - Routes: {len(route_performance)}\")\n",
    "            print(f\"   - Geometries: {len(route_geometries)}\")\n",
    "            print(f\"   - Locations: {len(location_analysis)}\")\n",
    "            print(f\"   - Total Delays: {summary_stats['total_delays']}\")\n",
    "            print(f\"   - Valid Delays: {summary_stats['valid_delays']}\")\n",
    "            print(f\"   - Average Delay: {summary_stats['avg_delay_minutes']} minutes\")\n",
    "            print(f\"\\nüìÅ Output folder: {self.output_data_folder}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nüí• Transformation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transformer = TTCDataTransformer()\n",
    "    success = transformer.transform_data()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n‚ú® Data update completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Data update failed!\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab0153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
