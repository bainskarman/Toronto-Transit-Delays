{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e409fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Failed to read Excel file: TTC Bus Delay Data since 2025\n",
      "   - Route: 426 unique values\n",
      "   - Line: 435 unique values\n",
      "üöç Final unique routes: 287\n",
      "   - Route: 426 unique values\n",
      "   - Line: 435 unique values\n",
      "üöç Final unique routes: 287\n",
      "   - Route: 426 unique values\n",
      "   - Line: 435 unique values\n",
      "üöç Final unique routes: 287\n",
      "üíæ Saving processed data...\n",
      "‚úÖ Saved route_performance.csv (193 routes)\n",
      "‚úÖ Saved route_geometries.json (222 routes)\n",
      "‚úÖ Saved location_analysis.csv (8333 locations)\n",
      "‚úÖ Saved summary_statistics.json\n",
      "\n",
      "‚ú® Data update completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "\n",
    "class TTCDataTransformer:\n",
    "    def __init__(self):\n",
    "        self.gtfs_package_id = \"b811ead4-6eaf-4adb-8408-d389fb5a069c\"\n",
    "        self.delay_package_id = \"e271cdae-8788-4980-96ce-6a5c95bc6618\"\n",
    "        self.base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca/api/3/action\"\n",
    "        \n",
    "        # Paths - fixed for Jupyter compatibility\n",
    "        try:\n",
    "            # This works when running as a script\n",
    "            self.script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        except NameError:\n",
    "            # This works in Jupyter notebooks\n",
    "            self.script_dir = os.getcwd()\n",
    "        \n",
    "        self.input_data_folder = os.path.join(self.script_dir, \"input_data\")\n",
    "        self.output_data_folder = os.path.join(self.script_dir, \"assets\", \"data\")\n",
    "        \n",
    "        # Create folders\n",
    "        self.ensure_folder_exists(self.input_data_folder)\n",
    "        self.ensure_folder_exists(self.output_data_folder)\n",
    "        \n",
    "        # Get current year for file filtering\n",
    "        self.current_year = datetime.now().year\n",
    "        \n",
    "        # Session for requests\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'TTC-Data-Transformer/1.0'\n",
    "        })\n",
    "\n",
    "    def ensure_folder_exists(self, folder_path):\n",
    "        \"\"\"Create folder if it doesn't exist\"\"\"\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            ##print(f\"üìÅ Created folder: {folder_path}\")\n",
    "\n",
    "    def fetch_package(self, package_id):\n",
    "        \"\"\"Fetch package information from CKAN API\"\"\"\n",
    "        url = f\"{self.base_url}/package_show?id={package_id}\"\n",
    "        response = self.session.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data.get('success'):\n",
    "            raise Exception(f\"API request failed: {data.get('error', {}).get('message', 'Unknown error')}\")\n",
    "        \n",
    "        return data['result']\n",
    "\n",
    "    def download_file(self, url, filepath):\n",
    "        \"\"\"Download file with progress tracking\"\"\"\n",
    "        #print(f\"üì• Downloading: {os.path.basename(filepath)}\")\n",
    "        response = self.session.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        downloaded_size = 0\n",
    "        \n",
    "        with open(filepath, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "                    downloaded_size += len(chunk)\n",
    "                    \n",
    "                    if total_size > 0:\n",
    "                        percent = (downloaded_size / total_size) * 100\n",
    "                        #print(f\"\\rüì• Progress: {percent:.1f}% ({self.format_file_size(downloaded_size)} / {self.format_file_size(total_size)})\", end=\"\")\n",
    "        \n",
    "        #print(\"\\n‚úÖ Download completed\")\n",
    "        return filepath\n",
    "\n",
    "    def format_file_size(self, size_bytes):\n",
    "        \"\"\"Format file size in human readable format\"\"\"\n",
    "        if size_bytes == 0:\n",
    "            return \"0B\"\n",
    "        \n",
    "        size_names = [\"B\", \"KB\", \"MB\", \"GB\"]\n",
    "        i = 0\n",
    "        while size_bytes >= 1024 and i < len(size_names)-1:\n",
    "            size_bytes /= 1024.0\n",
    "            i += 1\n",
    "        \n",
    "        return f\"{size_bytes:.2f} {size_names[i]}\"\n",
    "\n",
    "    def extract_year_from_filename(self, filename):\n",
    "        \"\"\"Extract year from filename for date filtering\"\"\"\n",
    "        # Look for 4-digit years in filename\n",
    "        years = re.findall(r'\\b(20\\d{2})\\b', filename)\n",
    "        if years:\n",
    "            return int(years[0])\n",
    "        return None\n",
    "\n",
    "    def download_delay_data(self):\n",
    "        \"\"\"Download ALL TTC Bus Delay Data with year-based format handling\"\"\"\n",
    "        #print(\"üöå Downloading ALL TTC Bus Delay Data...\")\n",
    "        \n",
    "        package_info = self.fetch_package(self.delay_package_id)\n",
    "        #print(f\"üì¶ Package: {package_info['title']}\")\n",
    "        \n",
    "        # NEW: Separate resources by year and format\n",
    "        excel_resources = []\n",
    "        csv_resources = []\n",
    "        \n",
    "        for resource in package_info['resources']:\n",
    "            resource_name = resource.get('name', '').lower()\n",
    "            resource_format = resource.get('format', '').lower()\n",
    "            \n",
    "            # Extract year from filename\n",
    "            file_year = self.extract_year_from_filename(resource_name)\n",
    "            if not file_year:\n",
    "                #print(f\"‚ö†Ô∏è Could not extract year from: {resource_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Check if it's delay data\n",
    "            if 'delay' in resource_name and 'ttc' in resource_name:\n",
    "                # For current year, prefer CSV files\n",
    "                if file_year == self.current_year:\n",
    "                    if 'csv' in resource_format:\n",
    "                        csv_resources.append(resource)\n",
    "                        #print(f\"‚úÖ Found CSV for current year {file_year}: {resource_name}\")\n",
    "                    elif 'xlsx' in resource_format or 'xls' in resource_format:\n",
    "                        excel_resources.append(resource)\n",
    "                        #print(f\"‚ÑπÔ∏è Found Excel for current year {file_year}: {resource_name}\")\n",
    "                # For previous years (2014 to current_year-1), prefer Excel files\n",
    "                elif 2014 <= file_year < self.current_year:\n",
    "                    if 'xlsx' in resource_format or 'xls' in resource_format:\n",
    "                        excel_resources.append(resource)\n",
    "                        #print(f\"‚úÖ Found Excel for year {file_year}: {resource_name}\")\n",
    "                    elif 'csv' in resource_format:\n",
    "                        csv_resources.append(resource)\n",
    "                        #print(f\"‚ÑπÔ∏è Found CSV for year {file_year}: {resource_name}\")\n",
    "        \n",
    "        #print(f\"\\nüìä File Summary:\")\n",
    "        #print(f\"   - Excel files (2014-{self.current_year-1}): {len(excel_resources)}\")\n",
    "        #print(f\"   - CSV files (current year {self.current_year}): {len(csv_resources)}\")\n",
    "        \n",
    "        if not excel_resources and not csv_resources:\n",
    "            raise Exception(\"No delay files found in package\")\n",
    "        \n",
    "        # Download and process each file\n",
    "        all_delay_data = []\n",
    "        \n",
    "        # Process Excel files first (historical data)\n",
    "        for i, resource in enumerate(excel_resources, 1):\n",
    "            #print(f\"\\nüìÅ Processing Excel file {i}/{len(excel_resources)}: {resource['name']}\")\n",
    "            \n",
    "            try:\n",
    "                # Download Excel file\n",
    "                excel_path = os.path.join(self.input_data_folder, f\"delay_data_excel_{i}.xlsx\")\n",
    "                self.download_file(resource['url'], excel_path)\n",
    "                \n",
    "                # Read Excel file with multiple engine attempts\n",
    "                #print(f\"üîç Reading Excel file: {resource['name']}\")\n",
    "                excel_data = self.read_excel_file(excel_path)\n",
    "                \n",
    "                if excel_data is not None:\n",
    "                    # Add source file information\n",
    "                    excel_data['source_file'] = resource['name']\n",
    "                    excel_data['file_year'] = self.extract_year_from_filename(resource['name'])\n",
    "                    \n",
    "                    #print(f\"‚úÖ Loaded {len(excel_data)} records from {resource['name']}\")\n",
    "                    #print(f\"   Columns: {list(excel_data.columns)}\")\n",
    "                    #print(f\"   Shape: {excel_data.shape}\")\n",
    "                    \n",
    "                    # Convert to list of dictionaries\n",
    "                    file_records = excel_data.to_dict('records')\n",
    "                    all_delay_data.extend(file_records)\n",
    "                    \n",
    "                    #print(f\"üìà Total records so far: {len(all_delay_data)}\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to read Excel file: {resource['name']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                #print(f\"‚ùå Error processing Excel {resource['name']}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Process CSV files (current year data)\n",
    "        for i, resource in enumerate(csv_resources, 1):\n",
    "            #print(f\"\\nüìÅ Processing CSV file {i}/{len(csv_resources)}: {resource['name']}\")\n",
    "            \n",
    "            try:\n",
    "                # Download CSV file\n",
    "                csv_path = os.path.join(self.input_data_folder, f\"delay_data_csv_{i}.csv\")\n",
    "                self.download_file(resource['url'], csv_path)\n",
    "                \n",
    "                # Read CSV file\n",
    "                #print(f\"üîç Reading CSV file: {resource['name']}\")\n",
    "                csv_data = pd.read_csv(csv_path, encoding='utf-8', low_memory=False)\n",
    "                \n",
    "                # Add source file information\n",
    "                csv_data['source_file'] = resource['name']\n",
    "                csv_data['file_year'] = self.extract_year_from_filename(resource['name'])\n",
    "                \n",
    "                #print(f\"‚úÖ Loaded {len(csv_data)} records from {resource['name']}\")\n",
    "                #print(f\"   Columns: {list(csv_data.columns)}\")\n",
    "                #print(f\"   Shape: {csv_data.shape}\")\n",
    "                \n",
    "                # Convert to list of dictionaries\n",
    "                file_records = csv_data.to_dict('records')\n",
    "                all_delay_data.extend(file_records)\n",
    "                \n",
    "                #print(f\"üìà Total records so far: {len(all_delay_data)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                #print(f\"‚ùå Error processing CSV {resource['name']}: {e}\")\n",
    "                # Try with different encoding\n",
    "                try:\n",
    "                    #print(\"üîÑ Trying with different encoding...\")\n",
    "                    csv_data = pd.read_csv(csv_path, encoding='latin-1', low_memory=False)\n",
    "                    \n",
    "                    # Add source file information\n",
    "                    csv_data['source_file'] = resource['name']\n",
    "                    csv_data['file_year'] = self.extract_year_from_filename(resource['name'])\n",
    "                    \n",
    "                    #print(f\"‚úÖ Loaded {len(csv_data)} records from {resource['name']} with latin-1 encoding\")\n",
    "                    \n",
    "                    # Convert to list of dictionaries\n",
    "                    file_records = csv_data.to_dict('records')\n",
    "                    all_delay_data.extend(file_records)\n",
    "                    \n",
    "                except Exception as e2:\n",
    "                    #print(f\"‚ùå Failed to read CSV with alternative encoding: {e2}\")\n",
    "                    continue\n",
    "        \n",
    "        #print(f\"\\nüéØ Successfully processed {len(excel_resources) + len(csv_resources)} files\")\n",
    "        #print(f\"üìä Total records merged: {len(all_delay_data)}\")\n",
    "        \n",
    "        # Save merged raw data\n",
    "        merged_data_path = os.path.join(self.input_data_folder, \"all_delay_data_merged.json\")\n",
    "        with open(merged_data_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_delay_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        #print(f\"üíæ Saved merged raw data to: {merged_data_path}\")\n",
    "        \n",
    "        return all_delay_data\n",
    "\n",
    "    def read_excel_file(self, file_path):\n",
    "        \"\"\"Read Excel file with multiple engine attempts\"\"\"\n",
    "        engines_to_try = ['openpyxl', 'xlrd']\n",
    "        \n",
    "        for engine in engines_to_try:\n",
    "            try:\n",
    "                #print(f\"   Trying engine: {engine}\")\n",
    "                data = pd.read_excel(file_path, engine=engine)\n",
    "                #print(f\"   ‚úÖ Success with engine: {engine}\")\n",
    "                return data\n",
    "            except Exception as e:\n",
    "                #print(f\"   ‚ùå Failed with engine {engine}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # If all engines fail, try without specifying engine\n",
    "        try:\n",
    "            #print(\"   Trying without engine specification...\")\n",
    "            data = pd.read_excel(file_path)\n",
    "            #print(\"   ‚úÖ Success without engine specification\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            #print(f\"   ‚ùå All attempts failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def download_gtfs_data(self):\n",
    "        \"\"\"Download and extract GTFS data\"\"\"\n",
    "        #print(\"üó∫Ô∏è Downloading GTFS Data...\")\n",
    "        \n",
    "        package_info = self.fetch_package(self.gtfs_package_id)\n",
    "        #print(f\"üì¶ Package: {package_info['title']}\")\n",
    "        \n",
    "        # Find the Complete GTFS resource\n",
    "        gtfs_resource = None\n",
    "        for resource in package_info['resources']:\n",
    "            if ('complete gtfs' in resource.get('name', '').lower() or \n",
    "                'completegtfs' in resource.get('name', '').lower()):\n",
    "                gtfs_resource = resource\n",
    "                break\n",
    "        \n",
    "        if not gtfs_resource:\n",
    "            raise Exception(\"Complete GTFS resource not found\")\n",
    "        \n",
    "        #print(f\"üì• Downloading GTFS ZIP from: {gtfs_resource['url']}\")\n",
    "        \n",
    "        # Download GTFS ZIP\n",
    "        zip_path = os.path.join(self.input_data_folder, \"complete_gtfs.zip\")\n",
    "        self.download_file(gtfs_resource['url'], zip_path)\n",
    "        \n",
    "        # Extract GTFS files\n",
    "        #print(\"üîß Extracting GTFS files...\")\n",
    "        gtfs_data = self.extract_gtfs_files(zip_path)\n",
    "        \n",
    "        return gtfs_data\n",
    "\n",
    "    def extract_gtfs_files(self, zip_path):\n",
    "        \"\"\"Extract required files from GTFS ZIP\"\"\"\n",
    "        gtfs_data = {}\n",
    "        required_files = ['routes.txt', 'trips.txt', 'shapes.txt', 'stops.txt']\n",
    "        \n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                # List files in ZIP\n",
    "                file_list = zip_ref.namelist()\n",
    "                #print(f\"üìÅ Files in GTFS ZIP: {len(file_list)}\")\n",
    "                \n",
    "                # Extract required files\n",
    "                for filename in required_files:\n",
    "                    if filename in file_list:\n",
    "                        # Extract file content\n",
    "                        with zip_ref.open(filename) as file:\n",
    "                            content = file.read().decode('utf-8')\n",
    "                            gtfs_data[filename] = content\n",
    "                            \n",
    "                        # Save individual file\n",
    "                        file_path = os.path.join(self.input_data_folder, filename)\n",
    "                        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                            f.write(content)\n",
    "                        \n",
    "                        #print(f\"‚úÖ Extracted: {filename}\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è Missing: {filename}\")\n",
    "            \n",
    "            return gtfs_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            #print(f\"‚ùå Error extracting GTFS files: {e}\")\n",
    "            raise\n",
    "\n",
    "    def safe_min_max(self, series):\n",
    "        \"\"\"Safely get min and max values for a series, handling mixed data types\"\"\"\n",
    "        try:\n",
    "            # Try to convert to numeric first\n",
    "            numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "            if not numeric_series.isna().all():\n",
    "                valid_values = numeric_series.dropna()\n",
    "                if len(valid_values) > 0:\n",
    "                    return f\"{valid_values.min()} to {valid_values.max()}\"\n",
    "            \n",
    "            # Try datetime\n",
    "            datetime_series = pd.to_datetime(series, errors='coerce')\n",
    "            if not datetime_series.isna().all():\n",
    "                valid_dates = datetime_series.dropna()\n",
    "                if len(valid_dates) > 0:\n",
    "                    return f\"{valid_dates.min()} to {valid_dates.max()}\"\n",
    "            \n",
    "            # For object types, show sample instead\n",
    "            unique_count = series.nunique()\n",
    "            sample_values = series.dropna().unique()[:3]\n",
    "            return f\"[Object type - {unique_count} unique values] Sample: {sample_values}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"[Error: {str(e)}]\"\n",
    "\n",
    "    def clean_delay_data(self, delay_data):\n",
    "        \"\"\"Clean and convert delay data types with detailed debugging\"\"\"\n",
    "        #print(\"üßπ Cleaning delay data types...\")\n",
    "        \n",
    "        df = pd.DataFrame(delay_data)\n",
    "        \n",
    "        # NEW: #print the full DataFrame info before processing\n",
    "        #print(\"\\n\" + \"=\"*80)\n",
    "        #print(\"üìä FULL DELAY DATA DATAFRAME INFO:\")\n",
    "        #print(\"=\"*80)\n",
    "        #print(f\"üìà DataFrame shape: {df.shape}\")\n",
    "        #print(f\"üìã Columns: {list(df.columns)}\")\n",
    "        \n",
    "        #print(\"\\nüîç Column details:\")\n",
    "        for col in df.columns:\n",
    "            #print(f\"   - {col}: {df[col].dtype}, {df[col].notna().sum()} non-null values\")\n",
    "            if df[col].dtype == 'object':\n",
    "                sample_values = df[col].dropna().unique()[:3]\n",
    "                #print(f\"     Sample values: {sample_values}\")\n",
    "        \n",
    "        # Check for date columns and their ranges - USE SAFE METHOD\n",
    "        date_columns = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "        #print(f\"\\nüìÖ Date-related columns: {date_columns}\")\n",
    "        \n",
    "        for date_col in date_columns:\n",
    "            if date_col in df.columns:\n",
    "                range_info = self.safe_min_max(df[date_col])\n",
    "                #print(f\"   - {date_col}: {range_info}\")\n",
    "        \n",
    "        # Check for route/line columns\n",
    "        route_columns = [col for col in df.columns if 'route' in col.lower() or 'line' in col.lower()]\n",
    "        #print(f\"\\nüöç Route/Line columns: {route_columns}\")\n",
    "        \n",
    "        for route_col in route_columns:\n",
    "            if route_col in df.columns:\n",
    "                print(f\"   - {route_col}: {df[route_col].nunique()} unique values\")\n",
    "        \n",
    "        # Check for delay columns\n",
    "        delay_columns = [col for col in df.columns if 'delay' in col.lower()]\n",
    "        #print(f\"\\n‚è±Ô∏è Delay columns: {delay_columns}\")\n",
    "        \n",
    "        for delay_col in delay_columns:\n",
    "            if delay_col in df.columns:\n",
    "                range_info = self.safe_min_max(df[delay_col])\n",
    "                #print(f\"   - {delay_col}: {range_info}\")\n",
    "        \n",
    "        #print(\"\\n\" + \"=\"*80)\n",
    "        #print(\"üîß STARTING DATA CLEANING...\")\n",
    "        #print(\"=\"*80)\n",
    "\n",
    "        # Convert numeric columns\n",
    "        if 'Min Delay' in df.columns:\n",
    "            df['Min Delay'] = pd.to_numeric(df['Min Delay'], errors='coerce').fillna(0)\n",
    "            #print(f\"‚úÖ Converted Min Delay to numeric: {len(df[df['Min Delay'] > 0])} valid delays\")\n",
    "        \n",
    "        # Try alternative delay column names\n",
    "        for delay_col in ['Delay', 'Delay Minutes', 'Delay_Minutes']:\n",
    "            if delay_col in df.columns and 'Min Delay' not in df.columns:\n",
    "                df['Min Delay'] = pd.to_numeric(df[delay_col], errors='coerce').fillna(0)\n",
    "                #print(f\"‚úÖ Using '{delay_col}' as Min Delay: {len(df[df['Min Delay'] > 0])} valid delays\")\n",
    "                break\n",
    "        \n",
    "        if 'Min Gap' in df.columns:\n",
    "            df['Min Gap'] = pd.to_numeric(df['Min Gap'], errors='coerce').fillna(0)\n",
    "        \n",
    "        if 'Vehicle' in df.columns:\n",
    "            df['Vehicle'] = pd.to_numeric(df['Vehicle'], errors='coerce').fillna(0)\n",
    "        \n",
    "        # Convert date columns - try multiple date column names\n",
    "        date_column_used = None\n",
    "        for date_col in ['Date', 'Incident Date', 'Report Date', 'Date & Time']:\n",
    "            if date_col in df.columns:\n",
    "                df['Date'] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "                date_column_used = date_col\n",
    "                #print(f\"‚úÖ Using '{date_col}' as Date column\")\n",
    "                break\n",
    "        \n",
    "        if date_column_used:\n",
    "            date_range_info = self.safe_min_max(df['Date'])\n",
    "            #print(f\"üìÖ Date range after cleaning: {date_range_info}\")\n",
    "            if df['Date'].notna().any():\n",
    "                years = df['Date'].dt.year.dropna().unique()\n",
    "                #print(f\"üìÖ Years in data: {sorted(years)}\")\n",
    "        else:\n",
    "            #print(\"‚ö†Ô∏è No date column found\")\n",
    "            # Create a dummy date column if none exists\n",
    "            df['Date'] = pd.to_datetime('2023-01-01')\n",
    "        \n",
    "        # Use 'Line' as Route and 'Station' as Location\n",
    "        if 'Line' in df.columns:\n",
    "            df['Route'] = df['Line']\n",
    "            #print(f\"‚úÖ Using 'Line' column as Route: {df['Route'].nunique()} unique routes\")\n",
    "        elif 'Route' not in df.columns:\n",
    "            # Try to find alternative route columns\n",
    "            for route_col in ['Route Number', 'Route No', 'Route_ID']:\n",
    "                if route_col in df.columns:\n",
    "                    df['Route'] = df[route_col]\n",
    "                    #print(f\"‚úÖ Using '{route_col}' as Route: {df['Route'].nunique()} unique routes\")\n",
    "                    break\n",
    "        \n",
    "        if 'Station' in df.columns:\n",
    "            df['Location'] = df['Station']\n",
    "            #print(f\"‚úÖ Using 'Station' column as Location: {df['Location'].nunique()} unique locations\")\n",
    "        elif 'Location' not in df.columns:\n",
    "            # Try to find alternative location columns\n",
    "            for loc_col in ['Stop', 'Stop Name', 'Station Name', 'Location Name']:\n",
    "                if loc_col in df.columns:\n",
    "                    df['Location'] = df[loc_col]\n",
    "                    #print(f\"‚úÖ Using '{loc_col}' as Location: {df['Location'].nunique()} unique locations\")\n",
    "                    break\n",
    "        \n",
    "        # Clean route names - extract route numbers\n",
    "        if 'Route' in df.columns:\n",
    "            df['Route'] = df['Route'].astype(str)\n",
    "            # Extract route numbers (e.g., \"102 MARKHAM ROAD\" -> \"102\")\n",
    "            df['Route_Number'] = df['Route'].str.extract(r'^(\\d+)')\n",
    "            df['Route'] = df['Route_Number'].fillna(df['Route'])\n",
    "            #print(f\"‚úÖ Extracted route numbers: {df['Route'].nunique()} unique routes\")\n",
    "        \n",
    "        # NEW: #print final DataFrame info after cleaning\n",
    "        #print(\"\\n\" + \"=\"*80)\n",
    "        #print(\"‚úÖ CLEANED DATAFRAME SUMMARY:\")\n",
    "        #print(\"=\"*80)\n",
    "        #print(f\"üìà Final shape: {df.shape}\")\n",
    "        #print(f\"üìã Final columns: {list(df.columns)}\")\n",
    "        \n",
    "        if 'Date' in df.columns:\n",
    "            date_range_info = self.safe_min_max(df['Date'])\n",
    "            #print(f\"üìÖ Final date range: {date_range_info}\")\n",
    "            if df['Date'].notna().any():\n",
    "                year_counts = df['Date'].dt.year.value_counts().sort_index()\n",
    "                #print(f\"üìÖ Records by year:\\n{year_counts}\")\n",
    "        \n",
    "        if 'Route' in df.columns:\n",
    "            print(f\"üöç Final unique routes: {df['Route'].nunique()}\")\n",
    "        \n",
    "        if 'Min Delay' in df.columns:\n",
    "            valid_delays = len(df[df['Min Delay'] > 0])\n",
    "            #print(f\"‚è±Ô∏è Valid delays (>0 min): {valid_delays}/{len(df)} ({valid_delays/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        #print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def process_route_performance(self, delay_data):\n",
    "        \"\"\"Process delay data into route performance metrics\"\"\"\n",
    "        #print(\"üìà Processing route performance data...\")\n",
    "        \n",
    "        # Clean and convert data types\n",
    "        df = self.clean_delay_data(delay_data)\n",
    "        \n",
    "        # Check if we have route data\n",
    "        if 'Route' not in df.columns:\n",
    "            #print(\"‚ùå No 'Route' column found in delay data\")\n",
    "            # Try to find alternative column names\n",
    "            for col in df.columns:\n",
    "                if 'route' in col.lower() or 'line' in col.lower():\n",
    "                    df['Route'] = df[col]\n",
    "                    #print(f\"‚úÖ Using '{col}' as Route column\")\n",
    "                    break\n",
    "        \n",
    "        if 'Route' not in df.columns:\n",
    "            #print(\"‚ùå No route data available\")\n",
    "            return []\n",
    "        \n",
    "        # Ensure Route column is string\n",
    "        df['Route'] = df['Route'].astype(str)\n",
    "        \n",
    "        # Filter out routes with no valid delays\n",
    "        df_valid = df[df['Min Delay'] > 0]\n",
    "        \n",
    "        if len(df_valid) == 0:\n",
    "            #print(\"‚ö†Ô∏è No valid delays found\")\n",
    "            return []\n",
    "        \n",
    "        # Group by route and calculate metrics\n",
    "        route_groups = df_valid.groupby('Route').agg({\n",
    "            'Min Delay': ['count', 'mean', 'sum'],\n",
    "            'Vehicle': 'nunique'\n",
    "        }).round(2)\n",
    "        \n",
    "        # Flatten column names\n",
    "        route_groups.columns = ['Delay_Count', 'Avg_Delay_Min', 'Total_Delay_Min', 'Unique_Vehicles']\n",
    "        route_groups = route_groups.reset_index()\n",
    "        \n",
    "        # NEW: Apply filters - only routes with more than 10 delays and exclude routes 1-4\n",
    "        #print(\"üîç Applying filters: routes with >10 delays and excluding routes 1-4\")\n",
    "        route_groups = route_groups[\n",
    "            (route_groups['Delay_Count'] > 10) & \n",
    "            (~route_groups['Route'].isin(['1', '2', '3', '4']))\n",
    "        ]\n",
    "        \n",
    "        #print(f\"üìä After filtering: {len(route_groups)} routes remaining\")\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        total_days = df['Date'].nunique() if 'Date' in df.columns and df['Date'].notna().any() else 30\n",
    "        route_groups['Delays_Per_Day'] = (route_groups['Delay_Count'] / total_days).round(2)\n",
    "        route_groups['On_Time_Percentage'] = 0  # Would need schedule data\n",
    "        \n",
    "        # Add route names\n",
    "        route_groups['route_long_name'] = route_groups['Route'].apply(lambda x: f\"Route {x}\")\n",
    "        \n",
    "        # Convert to list of dictionaries\n",
    "        route_performance = route_groups.to_dict('records')\n",
    "        \n",
    "        #print(f\"‚úÖ Processed {len(route_performance)} routes (filtered: >10 delays, excluding 1-4)\")\n",
    "        return route_performance\n",
    "\n",
    "    def process_route_geometries(self, gtfs_data):\n",
    "        \"\"\"Process GTFS data into route geometries\"\"\"\n",
    "        #print(\"üó∫Ô∏è Processing route geometries...\")\n",
    "        \n",
    "        route_geometries = {}\n",
    "        \n",
    "        try:\n",
    "            # Parse shapes data\n",
    "            if 'shapes.txt' in gtfs_data:\n",
    "                shapes_path = os.path.join(self.input_data_folder, 'shapes.txt')\n",
    "                trips_path = os.path.join(self.input_data_folder, 'trips.txt')\n",
    "                routes_path = os.path.join(self.input_data_folder, 'routes.txt')\n",
    "                \n",
    "                if (os.path.exists(shapes_path) and \n",
    "                    os.path.exists(trips_path) and \n",
    "                    os.path.exists(routes_path)):\n",
    "                    \n",
    "                    # Read with explicit dtype to avoid mixed type warnings\n",
    "                    shapes_df = pd.read_csv(shapes_path, dtype={'shape_id': str})\n",
    "                    trips_df = pd.read_csv(trips_path, dtype={'route_id': str, 'shape_id': str})\n",
    "                    routes_df = pd.read_csv(routes_path, dtype={'route_id': str})\n",
    "                    \n",
    "                    #print(f\"üìä Shapes: {len(shapes_df)}, Trips: {len(trips_df)}, Routes: {len(routes_df)}\")\n",
    "                    \n",
    "                    # Group shapes by shape_id\n",
    "                    shapes_by_route = {}\n",
    "                    for shape_id, group in shapes_df.groupby('shape_id'):\n",
    "                        # Sort by sequence and get coordinates\n",
    "                        coords = group.sort_values('shape_pt_sequence')[['shape_pt_lat', 'shape_pt_lon']].values.tolist()\n",
    "                        shapes_by_route[shape_id] = coords\n",
    "                    \n",
    "                    # Map routes to shapes via trips\n",
    "                    route_to_shape = {}\n",
    "                    for _, trip in trips_df.iterrows():\n",
    "                        if pd.notna(trip['route_id']) and pd.notna(trip['shape_id']):\n",
    "                            route_to_shape[trip['route_id']] = trip['shape_id']\n",
    "                    \n",
    "                    # Create geometries for each route\n",
    "                    for route_id, shape_id in route_to_shape.items():\n",
    "                        if shape_id in shapes_by_route:\n",
    "                            coordinates = []\n",
    "                            for lat, lon in shapes_by_route[shape_id]:\n",
    "                                if (isinstance(lat, (int, float)) and isinstance(lon, (int, float)) and\n",
    "                                    -90 <= lat <= 90 and -180 <= lon <= 180):\n",
    "                                    coordinates.append([float(lat), float(lon)])\n",
    "                            \n",
    "                            if coordinates:\n",
    "                                route_geometries[str(route_id)] = coordinates\n",
    "                    \n",
    "                    #print(f\"‚úÖ Processed {len(route_geometries)} route geometries from GTFS\")\n",
    "                else:\n",
    "                    #print(\"‚ö†Ô∏è GTFS files not found, generating sample geometries\")\n",
    "                    self.create_sample_geometries(route_geometries)\n",
    "            else:\n",
    "                #print(\"‚ö†Ô∏è No shapes.txt found, generating sample geometries\")\n",
    "                self.create_sample_geometries(route_geometries)\n",
    "                \n",
    "        except Exception as e:\n",
    "            #print(f\"‚ö†Ô∏è Error processing GTFS geometries: {e}\")\n",
    "            #print(\"üîÑ Generating sample geometries instead\")\n",
    "            self.create_sample_geometries(route_geometries)\n",
    "        \n",
    "        return route_geometries\n",
    "\n",
    "    def create_sample_geometries(self, route_geometries):\n",
    "        \"\"\"Create sample geometries when GTFS data is not available\"\"\"\n",
    "        toronto_center = [43.6532, -79.3832]\n",
    "        # Only include routes that would pass our filters (not 1-4)\n",
    "        routes = ['501', '504', '505', '506', '509', '510', '511', '512', '96', '165', '102', '35']\n",
    "        \n",
    "        for i, route in enumerate(routes):\n",
    "            coordinates = []\n",
    "            point_count = 8 + i\n",
    "            \n",
    "            for j in range(point_count):\n",
    "                angle = (j / point_count) * 3.14  # Semi-circle\n",
    "                lat = toronto_center[0] + (0.01 * i) + (0.005 * math.cos(angle))\n",
    "                lng = toronto_center[1] + (0.01 * i) + (0.005 * math.sin(angle))\n",
    "                coordinates.append([round(lat, 6), round(lng, 6)])\n",
    "            \n",
    "            route_geometries[route] = coordinates\n",
    "        \n",
    "        #print(f\"‚úÖ Generated {len(route_geometries)} sample route geometries\")\n",
    "\n",
    "    def process_location_analysis(self, delay_data):\n",
    "        \"\"\"Process delay data into location analysis\"\"\"\n",
    "        #print(\"üìç Processing location analysis...\")\n",
    "        \n",
    "        df = self.clean_delay_data(delay_data)\n",
    "        \n",
    "        # Check if we have location data\n",
    "        if 'Location' not in df.columns:\n",
    "            #print(\"‚ùå No 'Location' column found in delay data\")\n",
    "            # Try to find alternative column names\n",
    "            for col in df.columns:\n",
    "                if 'location' in col.lower() or 'station' in col.lower() or 'stop' in col.lower():\n",
    "                    df['Location'] = df[col]\n",
    "                    #print(f\"‚úÖ Using '{col}' as Location column\")\n",
    "                    break\n",
    "        \n",
    "        if 'Location' not in df.columns:\n",
    "            #print(\"‚ùå No location data available\")\n",
    "            return []\n",
    "        \n",
    "        # Filter out records without location\n",
    "        df_with_location = df[df['Location'].notna() & (df['Location'] != '') & (df['Location'] != 'Unknown')]\n",
    "        \n",
    "        if len(df_with_location) == 0:\n",
    "            #print(\"‚ö†Ô∏è No location data found\")\n",
    "            return []\n",
    "        \n",
    "        # Filter only records with valid delays\n",
    "        df_valid = df_with_location[df_with_location['Min Delay'] > 0]\n",
    "        \n",
    "        if len(df_valid) == 0:\n",
    "            #print(\"‚ö†Ô∏è No valid delays at locations found\")\n",
    "            return []\n",
    "        \n",
    "        # Group by location\n",
    "        location_groups = df_valid.groupby('Location').agg({\n",
    "            'Min Delay': ['count', 'mean'],\n",
    "            'Route': 'nunique',\n",
    "            'Vehicle': 'nunique'\n",
    "        }).round(2)\n",
    "        \n",
    "        # Flatten columns\n",
    "        location_groups.columns = ['total_delays', 'avg_delay_min', 'route_count', 'vehicle_count']\n",
    "        location_groups = location_groups.reset_index()\n",
    "        \n",
    "        # Convert to list of dictionaries\n",
    "        location_analysis = []\n",
    "        for _, row in location_groups.iterrows():\n",
    "            location_analysis.append({\n",
    "                'location_id': self.sanitize_location_id(row['Location']),\n",
    "                'location_name': row['Location'],\n",
    "                'total_delays': int(row['total_delays']),\n",
    "                'avg_delay_min': float(row['avg_delay_min']),\n",
    "                'latitude': self.generate_toronto_lat(),\n",
    "                'longitude': self.generate_toronto_lng(),\n",
    "                'route_count': int(row['route_count']),\n",
    "                'vehicle_count': int(row['vehicle_count']),\n",
    "                'peak_hours': json.dumps(['07:00-09:00', '16:00-18:00'])\n",
    "            })\n",
    "        \n",
    "        # Sort by total delays\n",
    "        location_analysis.sort(key=lambda x: x['total_delays'], reverse=True)\n",
    "        \n",
    "        #print(f\"‚úÖ Processed {len(location_analysis)} locations\")\n",
    "        return location_analysis\n",
    "\n",
    "    def sanitize_location_id(self, location_name):\n",
    "        \"\"\"Create a sanitized location ID\"\"\"\n",
    "        return (location_name.lower()\n",
    "                .replace(' ', '_')\n",
    "                .replace('/', '_')\n",
    "                .replace('\\\\', '_')\n",
    "                .replace('&', 'and')\n",
    "                .replace(\"'\", '')\n",
    "                .replace('\"', '')\n",
    "                .replace('(', '')\n",
    "                .replace(')', '')\n",
    "                .replace(',', '')[:50])\n",
    "\n",
    "    def generate_toronto_lat(self):\n",
    "        \"\"\"Generate random Toronto latitude\"\"\"\n",
    "        return round(43.65 + (random.random() - 0.5) * 0.1, 6)\n",
    "\n",
    "    def generate_toronto_lng(self):\n",
    "        \"\"\"Generate random Toronto longitude\"\"\"\n",
    "        return round(-79.38 + (random.random() - 0.5) * 0.1, 6)\n",
    "\n",
    "    def process_summary_statistics(self, delay_data, route_performance, location_analysis):\n",
    "        \"\"\"Calculate summary statistics\"\"\"\n",
    "        #print(\"üìä Processing summary statistics...\")\n",
    "        \n",
    "        df = self.clean_delay_data(delay_data)\n",
    "        \n",
    "        total_delays = len(delay_data)\n",
    "        \n",
    "        # Count valid delays (Min Delay > 0)\n",
    "        valid_delays = len(df[df['Min Delay'] > 0])\n",
    "        avg_delay = df[df['Min Delay'] > 0]['Min Delay'].mean() if valid_delays > 0 else 0\n",
    "        \n",
    "        # Count unique routes and vehicles\n",
    "        unique_routes = df['Route'].nunique() if 'Route' in df.columns else 0\n",
    "        unique_vehicles = df['Vehicle'].nunique() if 'Vehicle' in df.columns else 0\n",
    "        unique_locations = df['Location'].nunique() if 'Location' in df.columns else 0\n",
    "        \n",
    "        # Calculate date range for delays\n",
    "        oldest_date = None\n",
    "        most_recent_date = None\n",
    "        if 'Date' in df.columns and df['Date'].notna().any():\n",
    "            oldest_date = df['Date'].min()\n",
    "            most_recent_date = df['Date'].max()\n",
    "            #print(f\"üìÖ Date range found: {oldest_date} to {most_recent_date}\")\n",
    "        \n",
    "        # Calculate coverage percentage based on filtered routes vs total routes\n",
    "        total_routes_in_data = df['Route'].nunique() if 'Route' in df.columns else 0\n",
    "        displayed_routes = len(route_performance)\n",
    "        \n",
    "        if total_routes_in_data > 0:\n",
    "            coverage_percentage = round((displayed_routes / total_routes_in_data) * 100, 1)\n",
    "        else:\n",
    "            coverage_percentage = 0\n",
    "        \n",
    "        #print(f\"üìà Coverage calculation: {displayed_routes} displayed / {total_routes_in_data} total = {coverage_percentage}%\")\n",
    "        \n",
    "        # Find most delayed route\n",
    "        most_delayed_route = None\n",
    "        if route_performance:\n",
    "            most_delayed_route = max(route_performance, key=lambda x: x['Avg_Delay_Min'])\n",
    "        \n",
    "        # NEW: Calculate data period from actual data\n",
    "        data_period = \"Unknown\"\n",
    "        if oldest_date and most_recent_date:\n",
    "            oldest_year = oldest_date.year\n",
    "            most_recent_year = most_recent_date.year\n",
    "            if oldest_year == most_recent_year:\n",
    "                data_period = str(most_recent_year)\n",
    "            else:\n",
    "                data_period = f\"{oldest_year}-{most_recent_year}\"\n",
    "        \n",
    "        stats = {\n",
    "            'total_delays': total_delays,\n",
    "            'valid_delays': valid_delays,\n",
    "            'avg_delay_minutes': round(avg_delay, 2),\n",
    "            'unique_routes': unique_routes,\n",
    "            'unique_vehicles': unique_vehicles,\n",
    "            'unique_locations': unique_locations,\n",
    "            'data_points': total_delays,\n",
    "            'coverage_percentage': coverage_percentage,\n",
    "            'time_period': data_period,\n",
    "            'updated_at': datetime.now().isoformat(),\n",
    "            'data_refresh_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'data_oldest_date': oldest_date.isoformat() if oldest_date else None,\n",
    "            'data_most_recent_date': most_recent_date.isoformat() if most_recent_date else None,\n",
    "            'data_update_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'peak_delay_hour': self.calculate_peak_hour(df),\n",
    "            'most_delayed_route': f\"{most_delayed_route['Route']} - {most_delayed_route['route_long_name']}\" if most_delayed_route else 'Unknown',\n",
    "            'displayed_routes_count': displayed_routes,\n",
    "            'total_routes_count': total_routes_in_data,\n",
    "            'data_quality': {\n",
    "                'valid_delay_percentage': round((valid_delays / total_delays * 100), 2) if total_delays > 0 else 0,\n",
    "                'route_coverage': unique_routes,\n",
    "                'location_coverage': unique_locations,\n",
    "                'date_range_available': oldest_date is not None and most_recent_date is not None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        #print(\"‚úÖ Summary statistics calculated\")\n",
    "        #print(f\"   - Data Period: {data_period}\")\n",
    "        #print(f\"   - Coverage: {coverage_percentage}% ({displayed_routes}/{total_routes_in_data} routes)\")\n",
    "        #print(f\"   - Date Range: {oldest_date} to {most_recent_date}\" if oldest_date else \"   - No date range available\")\n",
    "        \n",
    "        return stats\n",
    "\n",
    "    def calculate_peak_hour(self, df):\n",
    "        \"\"\"Calculate peak delay hour from data\"\"\"\n",
    "        try:\n",
    "            if 'Time' in df.columns:\n",
    "                # Extract hour from time objects or strings\n",
    "                time_series = df['Time'].dropna()\n",
    "                if len(time_series) > 0:\n",
    "                    # Convert to string first, then extract hour\n",
    "                    time_strings = time_series.astype(str)\n",
    "                    # Parse hours from various time formats\n",
    "                    hours = []\n",
    "                    for time_str in time_strings:\n",
    "                        try:\n",
    "                            if ':' in time_str:\n",
    "                                hour_part = time_str.split(':')[0]\n",
    "                                hour = int(hour_part)\n",
    "                                hours.append(hour)\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    if hours:\n",
    "                        hour_series = pd.Series(hours)\n",
    "                        peak_hour = int(hour_series.mode().iloc[0]) if not hour_series.mode().empty else 8\n",
    "                        return f\"{peak_hour:02d}:00\"\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error calculating peak hour: {e}\")\n",
    "        \n",
    "        return \"08:00\"  # Fallback\n",
    "\n",
    "    def save_processed_data(self, route_performance, route_geometries, location_analysis, summary_stats):\n",
    "        \"\"\"Save all processed data to output folder\"\"\"\n",
    "        print(\"üíæ Saving processed data...\")\n",
    "        \n",
    "        # Save route performance as CSV\n",
    "        route_performance_path = os.path.join(self.output_data_folder, \"route_performance.csv\")\n",
    "        with open(route_performance_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            if route_performance:\n",
    "                writer = csv.DictWriter(f, fieldnames=route_performance[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(route_performance)\n",
    "        print(f\"‚úÖ Saved route_performance.csv ({len(route_performance)} routes)\")\n",
    "        \n",
    "        # Save route geometries as JSON\n",
    "        route_geometries_path = os.path.join(self.output_data_folder, \"route_geometries.json\")\n",
    "        with open(route_geometries_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(route_geometries, f, indent=2)\n",
    "        print(f\"‚úÖ Saved route_geometries.json ({len(route_geometries)} routes)\")\n",
    "        \n",
    "        # Save location analysis as CSV\n",
    "        location_analysis_path = os.path.join(self.output_data_folder, \"location_analysis.csv\")\n",
    "        with open(location_analysis_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            if location_analysis:\n",
    "                writer = csv.DictWriter(f, fieldnames=location_analysis[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(location_analysis)\n",
    "        print(f\"‚úÖ Saved location_analysis.csv ({len(location_analysis)} locations)\")\n",
    "        \n",
    "        # Save summary statistics as JSON\n",
    "        summary_stats_path = os.path.join(self.output_data_folder, \"summary_statistics.json\")\n",
    "        with open(summary_stats_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary_stats, f, indent=2, default=str)\n",
    "        print(\"‚úÖ Saved summary_statistics.json\")\n",
    "    \n",
    "\n",
    "\n",
    "    def should_update_data(self):\n",
    "        \"\"\"Check if data needs to be updated (older than 1 hour)\"\"\"\n",
    "        stats_file = os.path.join(self.output_data_folder, \"summary_statistics.json\")\n",
    "        \n",
    "        if not os.path.exists(stats_file):\n",
    "            return True\n",
    "        \n",
    "        try:\n",
    "            with open(stats_file, 'r', encoding='utf-8') as f:\n",
    "                stats = json.load(f)\n",
    "            \n",
    "            if 'updated_at' in stats:\n",
    "                last_updated = datetime.fromisoformat(stats['updated_at'].replace('Z', '+00:00'))\n",
    "                one_hour_ago = datetime.now() - timedelta(hours=1)\n",
    "                return last_updated < one_hour_ago\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def transform_data(self):\n",
    "        \"\"\"Main transformation function\"\"\"\n",
    "        #print(\"üîÑ Starting TTC Data Transformation...\")\n",
    "        #print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Check if update is needed\n",
    "            # if not self.should_update_data():\n",
    "            #     #print(\"üìä Data is recent (less than 1 hour old), skipping update\")\n",
    "            #     return True\n",
    "            \n",
    "            # Step 1: Download raw data\n",
    "            #print(\"\\nüì• Downloading raw data...\")\n",
    "            delay_data = self.download_delay_data()\n",
    "            gtfs_data = self.download_gtfs_data()\n",
    "            \n",
    "            #print(\"\\n‚úÖ Raw data downloaded successfully\")\n",
    "            #print(\"=\" * 50)\n",
    "            \n",
    "            # Step 2: Process data\n",
    "            #print(\"\\nüîß Processing data...\")\n",
    "            route_performance = self.process_route_performance(delay_data)\n",
    "            route_geometries = self.process_route_geometries(gtfs_data)\n",
    "            location_analysis = self.process_location_analysis(delay_data)\n",
    "            summary_stats = self.process_summary_statistics(delay_data, route_performance, location_analysis)\n",
    "            \n",
    "            #print(\"\\n‚úÖ Data processing completed\")\n",
    "            #print(\"=\" * 50)\n",
    "            \n",
    "            # Step 3: Save processed data\n",
    "            #print(\"\\nüíæ Saving processed data...\")\n",
    "            self.save_processed_data(route_performance, route_geometries, location_analysis, summary_stats)\n",
    "            \n",
    "            #print(\"\\nüéâ Transformation completed successfully!\")\n",
    "            #print(\"=\" * 50)\n",
    "            #print(\"üìä Summary:\")\n",
    "            #print(f\"   - Routes: {len(route_performance)} (filtered: >10 delays, excluding 1-4)\")\n",
    "            #print(f\"   - Geometries: {len(route_geometries)}\")\n",
    "            #print(f\"   - Locations: {len(location_analysis)}\")\n",
    "            #print(f\"   - Total Delays: {summary_stats['total_delays']}\")\n",
    "            #print(f\"   - Valid Delays: {summary_stats['valid_delays']}\")\n",
    "            #print(f\"   - Average Delay: {summary_stats['avg_delay_minutes']} minutes\")\n",
    "            #print(f\"   - Coverage: {summary_stats['coverage_percentage']}%\")\n",
    "            #print(f\"   - Data Period: {summary_stats['time_period']}\")\n",
    "            #print(f\"   - Date Range: {summary_stats.get('data_oldest_date', 'N/A')} to {summary_stats.get('data_most_recent_date', 'N/A')}\")\n",
    "            #print(f\"\\nüìÅ Output folder: {self.output_data_folder}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            #print(f\"\\nüí• Transformation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transformer = TTCDataTransformer()\n",
    "    success = transformer.transform_data()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n‚ú® Data update completed successfully!\")\n",
    "    else:\n",
    "        #print(\"\\n‚ùå Data update failed!\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8bd975d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Starting TTC Data Transformation...\n",
      "==================================================\n",
      "\n",
      "üì• Downloading raw data using new method...\n",
      "üöå Downloading ALL TTC Bus Delay Data (2014-2025)...\n",
      "üöå Loading and merging TTC Bus Delay Data (2014-2025)...\n",
      "üì• Downloading ttc-bus-delay-data-2014 (year=2014, type=xlsx)\n",
      "‚úÖ Loaded 9822 records from ttc-bus-delay-data-2014\n",
      "üì• Downloading ttc-bus-delay-data-2015 (year=2015, type=xlsx)\n",
      "‚úÖ Loaded 6665 records from ttc-bus-delay-data-2015\n",
      "üì• Downloading ttc-bus-delay-data-2016 (year=2016, type=xlsx)\n",
      "‚úÖ Loaded 5326 records from ttc-bus-delay-data-2016\n",
      "üì• Downloading ttc-bus-delay-data-2017 (year=2017, type=xlsx)\n",
      "‚úÖ Loaded 5300 records from ttc-bus-delay-data-2017\n",
      "üì• Downloading ttc-bus-delay-data-2018 (year=2018, type=xlsx)\n",
      "‚úÖ Loaded 6969 records from ttc-bus-delay-data-2018\n",
      "üì• Downloading ttc-bus-delay-data-2019 (year=2019, type=xlsx)\n",
      "‚úÖ Loaded 6743 records from ttc-bus-delay-data-2019\n",
      "üì• Downloading ttc-bus-delay-data-2020 (year=2020, type=xlsx)\n",
      "‚úÖ Loaded 4282 records from ttc-bus-delay-data-2020\n",
      "üì• Downloading ttc-bus-delay-data-2021 (year=2021, type=xlsx)\n",
      "‚úÖ Loaded 2832 records from ttc-bus-delay-data-2021\n",
      "üì• Downloading ttc-bus-delay-data-2022 (year=2022, type=xlsx)\n",
      "‚úÖ Loaded 58707 records from ttc-bus-delay-data-2022\n",
      "üì• Downloading ttc-bus-delay-data-2023 (year=2023, type=xlsx)\n",
      "‚úÖ Loaded 56207 records from ttc-bus-delay-data-2023\n",
      "üì• Downloading ttc-bus-delay-data-2024 (year=2024, type=xlsx)\n",
      "‚úÖ Loaded 59643 records from ttc-bus-delay-data-2024\n",
      "üì• Downloading TTC Bus Delay Data since 2025.csv (year=2025, type=csv)\n",
      "‚úÖ Loaded 46291 records from TTC Bus Delay Data since 2025.csv\n",
      "üìä Merged 12 files into 268787 total records\n",
      "üíæ Saved merged data to: /workspaces/Toronto-Transit-Delays/input_data/all_delay_data_merged.csv\n",
      "‚úÖ Successfully loaded 268787 records from 2014-2025\n",
      "üó∫Ô∏è Downloading GTFS Data...\n",
      "üì¶ Package: Merged GTFS - TTC Routes and Schedules\n",
      "üì• Downloading GTFS ZIP from: https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/b811ead4-6eaf-4adb-8408-d389fb5a069c/resource/c920e221-7a1c-488b-8c5b-6d8cd4e85eaf/download/completegtfs.zip\n",
      "üì• Downloading: complete_gtfs.zip\n",
      "üì• Progress: 100.0%\n",
      "‚úÖ Download completed\n",
      "üîß Extracting GTFS files...\n",
      "üìÅ Files in GTFS ZIP: 8\n",
      "‚úÖ Extracted: routes.txt\n",
      "‚úÖ Extracted: trips.txt\n",
      "‚úÖ Extracted: shapes.txt\n",
      "‚úÖ Extracted: stops.txt\n",
      "\n",
      "‚úÖ Raw data downloaded successfully\n",
      "==================================================\n",
      "\n",
      "üîß Processing data...\n",
      "üìà Processing route performance data...\n",
      "üßπ Cleaning delay data types...\n",
      "\n",
      "================================================================================\n",
      "üìä FULL DELAY DATA DATAFRAME INFO:\n",
      "================================================================================\n",
      "üìà DataFrame shape: (268787, 10)\n",
      "üìã Columns: ['Date', 'Route', 'Time', 'Day', 'Location', 'Incident', 'Min Delay', 'Min Gap', 'Direction', 'Vehicle']\n",
      "\n",
      "üîç Column details:\n",
      "   - Date: object, 268787 non-null values\n",
      "     Sample values: [Timestamp('2014-01-01 00:00:00') Timestamp('2014-01-02 00:00:00')\n",
      " Timestamp('2014-01-03 00:00:00')]\n",
      "   - Route: object, 266636 non-null values\n",
      "     Sample values: ['95' '102' '54']\n",
      "   - Time: object, 268787 non-null values\n",
      "     Sample values: [datetime.time(0, 23) datetime.time(0, 55) datetime.time(1, 28)]\n",
      "   - Day: object, 268787 non-null values\n",
      "     Sample values: ['Wednesday' 'Thursday' 'Friday']\n",
      "   - Location: object, 268710 non-null values\n",
      "     Sample values: ['York Mills station' 'Entire run for route' 'lawrence and Warden']\n",
      "   - Incident: object, 268787 non-null values\n",
      "     Sample values: ['Mechanical' 'General Delay' 'Emergency Services']\n",
      "   - Min Delay: float64, 268743 non-null values\n",
      "   - Min Gap: float64, 268710 non-null values\n",
      "   - Direction: object, 229264 non-null values\n",
      "     Sample values: ['E' 'b/w' 'WB']\n",
      "   - Vehicle: float64, 263805 non-null values\n",
      "\n",
      "üìÖ Date-related columns: ['Date', 'Time']\n",
      "   - Date: 2014-01-01 00:00:00 to 2025-09-30 00:00:00\n",
      "   - Time: 2017-01-26 00:00:00 to 2025-11-15 23:59:00\n",
      "\n",
      "üöç Route/Line columns: ['Route']\n",
      "   - Route: 475 unique values\n",
      "\n",
      "‚è±Ô∏è Delay columns: ['Min Delay']\n",
      "   - Min Delay: -30.0 to 1244.0\n",
      "\n",
      "================================================================================\n",
      "üîß STARTING DATA CLEANING...\n",
      "================================================================================\n",
      "‚úÖ Converted Min Delay to numeric: 248093 valid delays\n",
      "‚úÖ Using 'Date' as Date column\n",
      "üìÖ Date range after cleaning: 1388534400000000000 to 1759190400000000000\n",
      "üìÖ Years in data: [np.int32(2014), np.int32(2015), np.int32(2016), np.int32(2017), np.int32(2018), np.int32(2019), np.int32(2020), np.int32(2021), np.int32(2022), np.int32(2023), np.int32(2024), np.int32(2025)]\n",
      "‚úÖ Extracted route numbers: 476 unique routes\n",
      "\n",
      "================================================================================\n",
      "‚úÖ CLEANED DATAFRAME SUMMARY:\n",
      "================================================================================\n",
      "üìà Final shape: (268787, 11)\n",
      "üìã Final columns: ['Date', 'Route', 'Time', 'Day', 'Location', 'Incident', 'Min Delay', 'Min Gap', 'Direction', 'Vehicle', 'Route_Number']\n",
      "üìÖ Final date range: 1388534400000000000 to 1759190400000000000\n",
      "üìÖ Records by year:\n",
      "Date\n",
      "2014     9822\n",
      "2015     6665\n",
      "2016     5326\n",
      "2017     5300\n",
      "2018     6969\n",
      "2019     6743\n",
      "2020     4282\n",
      "2021     2832\n",
      "2022    58707\n",
      "2023    56207\n",
      "2024    59643\n",
      "2025    46291\n",
      "Name: count, dtype: int64\n",
      "üöç Final unique routes: 476\n",
      "‚è±Ô∏è Valid delays (>0 min): 248093/268787 (92.3%)\n",
      "\n",
      "================================================================================\n",
      "üîç Applying filters: routes with >10 delays and excluding routes 1-4\n",
      "üìä After filtering: 265 routes remaining\n",
      "‚úÖ Processed 265 routes (filtered: >10 delays, excluding 1-4)\n",
      "üó∫Ô∏è Processing route geometries...\n",
      "üìä Shapes: 983749, Trips: 101065, Routes: 222\n",
      "‚úÖ Processed 222 route geometries from GTFS\n",
      "üìç Processing location analysis...\n",
      "üßπ Cleaning delay data types...\n",
      "\n",
      "================================================================================\n",
      "üìä FULL DELAY DATA DATAFRAME INFO:\n",
      "================================================================================\n",
      "üìà DataFrame shape: (268787, 10)\n",
      "üìã Columns: ['Date', 'Route', 'Time', 'Day', 'Location', 'Incident', 'Min Delay', 'Min Gap', 'Direction', 'Vehicle']\n",
      "\n",
      "üîç Column details:\n",
      "   - Date: object, 268787 non-null values\n",
      "     Sample values: [Timestamp('2014-01-01 00:00:00') Timestamp('2014-01-02 00:00:00')\n",
      " Timestamp('2014-01-03 00:00:00')]\n",
      "   - Route: object, 266636 non-null values\n",
      "     Sample values: ['95' '102' '54']\n",
      "   - Time: object, 268787 non-null values\n",
      "     Sample values: [datetime.time(0, 23) datetime.time(0, 55) datetime.time(1, 28)]\n",
      "   - Day: object, 268787 non-null values\n",
      "     Sample values: ['Wednesday' 'Thursday' 'Friday']\n",
      "   - Location: object, 268710 non-null values\n",
      "     Sample values: ['York Mills station' 'Entire run for route' 'lawrence and Warden']\n",
      "   - Incident: object, 268787 non-null values\n",
      "     Sample values: ['Mechanical' 'General Delay' 'Emergency Services']\n",
      "   - Min Delay: float64, 268743 non-null values\n",
      "   - Min Gap: float64, 268710 non-null values\n",
      "   - Direction: object, 229264 non-null values\n",
      "     Sample values: ['E' 'b/w' 'WB']\n",
      "   - Vehicle: float64, 263805 non-null values\n",
      "\n",
      "üìÖ Date-related columns: ['Date', 'Time']\n",
      "   - Date: 2014-01-01 00:00:00 to 2025-09-30 00:00:00\n",
      "   - Time: 2017-01-26 00:00:00 to 2025-11-15 23:59:00\n",
      "\n",
      "üöç Route/Line columns: ['Route']\n",
      "   - Route: 475 unique values\n",
      "\n",
      "‚è±Ô∏è Delay columns: ['Min Delay']\n",
      "   - Min Delay: -30.0 to 1244.0\n",
      "\n",
      "================================================================================\n",
      "üîß STARTING DATA CLEANING...\n",
      "================================================================================\n",
      "‚úÖ Converted Min Delay to numeric: 248093 valid delays\n",
      "‚úÖ Using 'Date' as Date column\n",
      "üìÖ Date range after cleaning: 1388534400000000000 to 1759190400000000000\n",
      "üìÖ Years in data: [np.int32(2014), np.int32(2015), np.int32(2016), np.int32(2017), np.int32(2018), np.int32(2019), np.int32(2020), np.int32(2021), np.int32(2022), np.int32(2023), np.int32(2024), np.int32(2025)]\n",
      "‚úÖ Extracted route numbers: 476 unique routes\n",
      "\n",
      "================================================================================\n",
      "‚úÖ CLEANED DATAFRAME SUMMARY:\n",
      "================================================================================\n",
      "üìà Final shape: (268787, 11)\n",
      "üìã Final columns: ['Date', 'Route', 'Time', 'Day', 'Location', 'Incident', 'Min Delay', 'Min Gap', 'Direction', 'Vehicle', 'Route_Number']\n",
      "üìÖ Final date range: 1388534400000000000 to 1759190400000000000\n",
      "üìÖ Records by year:\n",
      "Date\n",
      "2014     9822\n",
      "2015     6665\n",
      "2016     5326\n",
      "2017     5300\n",
      "2018     6969\n",
      "2019     6743\n",
      "2020     4282\n",
      "2021     2832\n",
      "2022    58707\n",
      "2023    56207\n",
      "2024    59643\n",
      "2025    46291\n",
      "Name: count, dtype: int64\n",
      "üöç Final unique routes: 476\n",
      "‚è±Ô∏è Valid delays (>0 min): 248093/268787 (92.3%)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Processed 38810 locations\n",
      "üìä Processing summary statistics...\n",
      "üßπ Cleaning delay data types...\n",
      "\n",
      "================================================================================\n",
      "üìä FULL DELAY DATA DATAFRAME INFO:\n",
      "================================================================================\n",
      "üìà DataFrame shape: (268787, 10)\n",
      "üìã Columns: ['Date', 'Route', 'Time', 'Day', 'Location', 'Incident', 'Min Delay', 'Min Gap', 'Direction', 'Vehicle']\n",
      "\n",
      "üîç Column details:\n",
      "   - Date: object, 268787 non-null values\n",
      "     Sample values: [Timestamp('2014-01-01 00:00:00') Timestamp('2014-01-02 00:00:00')\n",
      " Timestamp('2014-01-03 00:00:00')]\n",
      "   - Route: object, 266636 non-null values\n",
      "     Sample values: ['95' '102' '54']\n",
      "   - Time: object, 268787 non-null values\n",
      "     Sample values: [datetime.time(0, 23) datetime.time(0, 55) datetime.time(1, 28)]\n",
      "   - Day: object, 268787 non-null values\n",
      "     Sample values: ['Wednesday' 'Thursday' 'Friday']\n",
      "   - Location: object, 268710 non-null values\n",
      "     Sample values: ['York Mills station' 'Entire run for route' 'lawrence and Warden']\n",
      "   - Incident: object, 268787 non-null values\n",
      "     Sample values: ['Mechanical' 'General Delay' 'Emergency Services']\n",
      "   - Min Delay: float64, 268743 non-null values\n",
      "   - Min Gap: float64, 268710 non-null values\n",
      "   - Direction: object, 229264 non-null values\n",
      "     Sample values: ['E' 'b/w' 'WB']\n",
      "   - Vehicle: float64, 263805 non-null values\n",
      "\n",
      "üìÖ Date-related columns: ['Date', 'Time']\n",
      "   - Date: 2014-01-01 00:00:00 to 2025-09-30 00:00:00\n",
      "   - Time: 2017-01-26 00:00:00 to 2025-11-15 23:59:00\n",
      "\n",
      "üöç Route/Line columns: ['Route']\n",
      "   - Route: 475 unique values\n",
      "\n",
      "‚è±Ô∏è Delay columns: ['Min Delay']\n",
      "   - Min Delay: -30.0 to 1244.0\n",
      "\n",
      "================================================================================\n",
      "üîß STARTING DATA CLEANING...\n",
      "================================================================================\n",
      "‚úÖ Converted Min Delay to numeric: 248093 valid delays\n",
      "‚úÖ Using 'Date' as Date column\n",
      "üìÖ Date range after cleaning: 1388534400000000000 to 1759190400000000000\n",
      "üìÖ Years in data: [np.int32(2014), np.int32(2015), np.int32(2016), np.int32(2017), np.int32(2018), np.int32(2019), np.int32(2020), np.int32(2021), np.int32(2022), np.int32(2023), np.int32(2024), np.int32(2025)]\n",
      "‚úÖ Extracted route numbers: 476 unique routes\n",
      "\n",
      "================================================================================\n",
      "‚úÖ CLEANED DATAFRAME SUMMARY:\n",
      "================================================================================\n",
      "üìà Final shape: (268787, 11)\n",
      "üìã Final columns: ['Date', 'Route', 'Time', 'Day', 'Location', 'Incident', 'Min Delay', 'Min Gap', 'Direction', 'Vehicle', 'Route_Number']\n",
      "üìÖ Final date range: 1388534400000000000 to 1759190400000000000\n",
      "üìÖ Records by year:\n",
      "Date\n",
      "2014     9822\n",
      "2015     6665\n",
      "2016     5326\n",
      "2017     5300\n",
      "2018     6969\n",
      "2019     6743\n",
      "2020     4282\n",
      "2021     2832\n",
      "2022    58707\n",
      "2023    56207\n",
      "2024    59643\n",
      "2025    46291\n",
      "Name: count, dtype: int64\n",
      "üöç Final unique routes: 476\n",
      "‚è±Ô∏è Valid delays (>0 min): 248093/268787 (92.3%)\n",
      "\n",
      "================================================================================\n",
      "üìÖ Date range found: 2014-01-01 00:00:00 to 2025-09-30 00:00:00\n",
      "üìà Coverage calculation: 265 displayed / 476 total = 55.7%\n",
      "‚úÖ Summary statistics calculated\n",
      "   - Data Period: 2014-2025\n",
      "   - Coverage: 55.7% (265/476 routes)\n",
      "   - Date Range: 2014-01-01 00:00:00 to 2025-09-30 00:00:00\n",
      "\n",
      "‚úÖ Data processing completed\n",
      "==================================================\n",
      "\n",
      "üíæ Saving processed data...\n",
      "üíæ Saving processed data...\n",
      "‚úÖ Saved route_performance.csv (265 routes)\n",
      "‚úÖ Saved route_geometries.json (222 routes)\n",
      "‚úÖ Saved location_analysis.csv (38810 locations)\n",
      "‚úÖ Saved summary_statistics.json\n",
      "\n",
      "üéâ Transformation completed successfully!\n",
      "==================================================\n",
      "üìä Summary:\n",
      "   - Routes: 265 (filtered: >10 delays, excluding 1-4)\n",
      "   - Geometries: 222\n",
      "   - Locations: 38810\n",
      "   - Total Delays: 268787\n",
      "   - Valid Delays: 248093\n",
      "   - Average Delay: 21.25 minutes\n",
      "   - Coverage: 55.7%\n",
      "   - Data Period: 2014-2025\n",
      "   - Date Range: 2014-01-01T00:00:00 to 2025-09-30T00:00:00\n",
      "\n",
      "üìÅ Output folder: /workspaces/Toronto-Transit-Delays/assets/data\n",
      "\n",
      "‚ú® Data update completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "class TTCDataTransformer:\n",
    "    def __init__(self):\n",
    "        self.gtfs_package_id = \"b811ead4-6eaf-4adb-8408-d389fb5a069c\"\n",
    "        self.delay_package_id = \"e271cdae-8788-4980-96ce-6a5c95bc6618\"\n",
    "        self.base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca/api/3/action\"\n",
    "        \n",
    "        # Paths - fixed for Jupyter compatibility\n",
    "        try:\n",
    "            # This works when running as a script\n",
    "            self.script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        except NameError:\n",
    "            # This works in Jupyter notebooks\n",
    "            self.script_dir = os.getcwd()\n",
    "        \n",
    "        self.input_data_folder = os.path.join(self.script_dir, \"input_data\")\n",
    "        self.output_data_folder = os.path.join(self.script_dir, \"assets\", \"data\")\n",
    "        \n",
    "        # Create folders\n",
    "        self.ensure_folder_exists(self.input_data_folder)\n",
    "        self.ensure_folder_exists(self.output_data_folder)\n",
    "        \n",
    "        # Get current year for file filtering\n",
    "        self.current_year = datetime.now().year\n",
    "        \n",
    "        # Session for requests\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'TTC-Data-Transformer/1.0'\n",
    "        })\n",
    "\n",
    "    def ensure_folder_exists(self, folder_path):\n",
    "        \"\"\"Create folder if it doesn't exist\"\"\"\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            ##print(f\"üìÅ Created folder: {folder_path}\")\n",
    "\n",
    "    def fetch_package(self, package_id):\n",
    "        \"\"\"Fetch package information from CKAN API\"\"\"\n",
    "        url = f\"{self.base_url}/package_show?id={package_id}\"\n",
    "        response = self.session.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data.get('success'):\n",
    "            raise Exception(f\"API request failed: {data.get('error', {}).get('message', 'Unknown error')}\")\n",
    "        \n",
    "        return data['result']\n",
    "\n",
    "    def clean_and_standardize(self, df):\n",
    "        \"\"\"Standardize column names and enforce a uniform schema.\"\"\"\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        rename_map = {\n",
    "            \"Report Date\": \"Date\",\n",
    "            \"Date\": \"Date\",\n",
    "            \"Time\": \"Time\",\n",
    "            \"Day\": \"Day\",\n",
    "            \"Location\": \"Location\",\n",
    "            \"Station\": \"Location\",  # 2025 CSV\n",
    "            \"Incident\": \"Incident\",\n",
    "            \"Code\": \"Incident\",     # 2025 CSV\n",
    "            \"Line\": \"Route\",        # 2025 CSV\n",
    "            \"Route\": \"Route\",\n",
    "            \"Min Delay\": \"Min Delay\",\n",
    "            \"Delay\": \"Min Delay\",   # 2020 XLSX\n",
    "            \"Min Gap\": \"Min Gap\",\n",
    "            \"Gap\": \"Min Gap\",       # 2020 XLSX\n",
    "            \"Direction\": \"Direction\",\n",
    "            \"Bound\": \"Direction\",   # 2025 CSV\n",
    "            \"Vehicle\": \"Vehicle\",\n",
    "        }\n",
    "\n",
    "        df = df.rename(columns=rename_map)\n",
    "\n",
    "        final_cols = [\n",
    "            \"Date\", \"Route\", \"Time\", \"Day\", \"Location\",\n",
    "            \"Incident\", \"Min Delay\", \"Min Gap\", \"Direction\", \"Vehicle\"\n",
    "        ]\n",
    "\n",
    "        for col in final_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = np.nan\n",
    "\n",
    "        df = df[final_cols]\n",
    "\n",
    "        # Step: Make Route column numeric only (keep digits)\n",
    "        def extract_digits(val):\n",
    "            if pd.isna(val):\n",
    "                return val\n",
    "            match = re.match(r\"(\\d+)\", str(val).strip())\n",
    "            return match.group(1) if match else val\n",
    "\n",
    "        df[\"Route\"] = df[\"Route\"].apply(extract_digits)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def load_and_merge_ttc_bus_delay_data_numeric_routes(self):\n",
    "        \"\"\"\n",
    "        NEW METHOD: Load TTC bus delay files, unify Route column as numeric only,\n",
    "        merge all years (2014-2025), and return DataFrame.\n",
    "        \"\"\"\n",
    "        print(\"üöå Loading and merging TTC Bus Delay Data (2014-2025)...\")\n",
    "        \n",
    "        base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca\"\n",
    "        package_id = \"ttc-bus-delay-data\"\n",
    "\n",
    "        # Get package metadata\n",
    "        package_url = f\"{base_url}/api/3/action/package_show\"\n",
    "        package = self.session.get(package_url, params={\"id\": package_id}).json()\n",
    "        resources = package[\"result\"][\"resources\"]\n",
    "\n",
    "        keyword = \"ttc\"\n",
    "        current_year = datetime.now().year\n",
    "        valid_years = set(range(2014, current_year + 1))\n",
    "        year_pattern = re.compile(r\"(19|20)\\d{2}\")\n",
    "\n",
    "        all_dfs = []\n",
    "\n",
    "        # Step 1: Load all files\n",
    "        for res in resources:\n",
    "            name = res.get(\"name\", \"\")\n",
    "            fmt = res.get(\"format\", \"\").lower()\n",
    "\n",
    "            if keyword not in name.lower():\n",
    "                continue\n",
    "\n",
    "            match = year_pattern.search(name)\n",
    "            if not match:\n",
    "                continue\n",
    "\n",
    "            year = int(match.group(0))\n",
    "            if year not in valid_years:\n",
    "                continue\n",
    "\n",
    "            expected_ext = \"xlsx\" if year < current_year else \"csv\"\n",
    "            if fmt != expected_ext:\n",
    "                continue\n",
    "\n",
    "            print(f\"üì• Downloading {name} (year={year}, type={fmt})\")\n",
    "            url = res[\"url\"]\n",
    "            response = self.session.get(url)\n",
    "\n",
    "            try:\n",
    "                if fmt == \"xlsx\":\n",
    "                    df = pd.read_excel(BytesIO(response.content))\n",
    "                else:\n",
    "                    df = pd.read_csv(StringIO(response.text))\n",
    "\n",
    "                # Clean + standardize + numeric route\n",
    "                df = self.clean_and_standardize(df)\n",
    "                all_dfs.append(df)\n",
    "                print(f\"‚úÖ Loaded {len(df)} records from {name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing {name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Step 2: Merge all datasets\n",
    "        if not all_dfs:\n",
    "            raise Exception(\"No valid delay data files found\")\n",
    "            \n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"üìä Merged {len(all_dfs)} files into {len(final_df)} total records\")\n",
    "\n",
    "        # Step 3: Save merged data for debugging\n",
    "        merged_data_path = os.path.join(self.input_data_folder, \"all_delay_data_merged.csv\")\n",
    "        final_df.to_csv(merged_data_path, index=False)\n",
    "        print(f\"üíæ Saved merged data to: {merged_data_path}\")\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def download_delay_data(self):\n",
    "        \"\"\"Download ALL TTC Bus Delay Data using the new method\"\"\"\n",
    "        print(\"üöå Downloading ALL TTC Bus Delay Data (2014-2025)...\")\n",
    "        \n",
    "        try:\n",
    "            # Use the new method to get merged DataFrame\n",
    "            df = self.load_and_merge_ttc_bus_delay_data_numeric_routes()\n",
    "            \n",
    "            # Convert to list of dictionaries for compatibility with existing code\n",
    "            all_delay_data = df.to_dict('records')\n",
    "            \n",
    "            # Also save as JSON for existing code compatibility\n",
    "            merged_json_path = os.path.join(self.input_data_folder, \"all_delay_data_merged.json\")\n",
    "            with open(merged_json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(all_delay_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "            \n",
    "            print(f\"‚úÖ Successfully loaded {len(all_delay_data)} records from 2014-2025\")\n",
    "            return all_delay_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in new download method: {e}\")\n",
    "            print(\"üîÑ Falling back to original download method...\")\n",
    "            return self.download_delay_data_fallback()\n",
    "\n",
    "    def download_delay_data_fallback(self):\n",
    "        \"\"\"Fallback method using original download logic\"\"\"\n",
    "        print(\"üîÑ Using fallback download method...\")\n",
    "        \n",
    "        package_info = self.fetch_package(self.delay_package_id)\n",
    "        print(f\"üì¶ Package: {package_info['title']}\")\n",
    "        \n",
    "        excel_resources = []\n",
    "        csv_resources = []\n",
    "        \n",
    "        for resource in package_info['resources']:\n",
    "            resource_name = resource.get('name', '').lower()\n",
    "            resource_format = resource.get('format', '').lower()\n",
    "            \n",
    "            file_year = self.extract_year_from_filename(resource_name)\n",
    "            if not file_year:\n",
    "                continue\n",
    "            \n",
    "            if 'delay' in resource_name and 'ttc' in resource_name:\n",
    "                if file_year == self.current_year:\n",
    "                    if 'csv' in resource_format:\n",
    "                        csv_resources.append(resource)\n",
    "                    elif 'xlsx' in resource_format or 'xls' in resource_format:\n",
    "                        excel_resources.append(resource)\n",
    "                elif 2014 <= file_year < self.current_year:\n",
    "                    if 'xlsx' in resource_format or 'xls' in resource_format:\n",
    "                        excel_resources.append(resource)\n",
    "                    elif 'csv' in resource_format:\n",
    "                        csv_resources.append(resource)\n",
    "        \n",
    "        print(f\"üìä File Summary:\")\n",
    "        print(f\"   - Excel files (2014-{self.current_year-1}): {len(excel_resources)}\")\n",
    "        print(f\"   - CSV files (current year {self.current_year}): {len(csv_resources)}\")\n",
    "        \n",
    "        if not excel_resources and not csv_resources:\n",
    "            raise Exception(\"No delay files found in package\")\n",
    "        \n",
    "        all_delay_data = []\n",
    "        \n",
    "        # Process Excel files\n",
    "        for i, resource in enumerate(excel_resources, 1):\n",
    "            print(f\"üìÅ Processing Excel file {i}/{len(excel_resources)}: {resource['name']}\")\n",
    "            \n",
    "            try:\n",
    "                excel_path = os.path.join(self.input_data_folder, f\"delay_data_excel_{i}.xlsx\")\n",
    "                self.download_file(resource['url'], excel_path)\n",
    "                excel_data = self.read_excel_file(excel_path)\n",
    "                \n",
    "                if excel_data is not None:\n",
    "                    excel_data['source_file'] = resource['name']\n",
    "                    excel_data['file_year'] = self.extract_year_from_filename(resource['name'])\n",
    "                    file_records = excel_data.to_dict('records')\n",
    "                    all_delay_data.extend(file_records)\n",
    "                    print(f\"‚úÖ Loaded {len(excel_data)} records from {resource['name']}\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to read Excel file: {resource['name']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing Excel {resource['name']}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Process CSV files\n",
    "        for i, resource in enumerate(csv_resources, 1):\n",
    "            print(f\"üìÅ Processing CSV file {i}/{len(csv_resources)}: {resource['name']}\")\n",
    "            \n",
    "            try:\n",
    "                csv_path = os.path.join(self.input_data_folder, f\"delay_data_csv_{i}.csv\")\n",
    "                self.download_file(resource['url'], csv_path)\n",
    "                csv_data = pd.read_csv(csv_path, encoding='utf-8', low_memory=False)\n",
    "                csv_data['source_file'] = resource['name']\n",
    "                csv_data['file_year'] = self.extract_year_from_filename(resource['name'])\n",
    "                file_records = csv_data.to_dict('records')\n",
    "                all_delay_data.extend(file_records)\n",
    "                print(f\"‚úÖ Loaded {len(csv_data)} records from {resource['name']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing CSV {resource['name']}: {e}\")\n",
    "                try:\n",
    "                    csv_data = pd.read_csv(csv_path, encoding='latin-1', low_memory=False)\n",
    "                    csv_data['source_file'] = resource['name']\n",
    "                    csv_data['file_year'] = self.extract_year_from_filename(resource['name'])\n",
    "                    file_records = csv_data.to_dict('records')\n",
    "                    all_delay_data.extend(file_records)\n",
    "                    print(f\"‚úÖ Loaded {len(csv_data)} records with latin-1 encoding\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"‚ùå Failed to read CSV with alternative encoding: {e2}\")\n",
    "                    continue\n",
    "        \n",
    "        print(f\"üéØ Successfully processed {len(excel_resources) + len(csv_resources)} files\")\n",
    "        print(f\"üìä Total records merged: {len(all_delay_data)}\")\n",
    "        \n",
    "        merged_data_path = os.path.join(self.input_data_folder, \"all_delay_data_merged.json\")\n",
    "        with open(merged_data_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_delay_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        print(f\"üíæ Saved merged raw data to: {merged_data_path}\")\n",
    "        return all_delay_data\n",
    "\n",
    "    def extract_year_from_filename(self, filename):\n",
    "        \"\"\"Extract year from filename for date filtering\"\"\"\n",
    "        years = re.findall(r'\\b(20\\d{2})\\b', filename)\n",
    "        if years:\n",
    "            return int(years[0])\n",
    "        return None\n",
    "\n",
    "    def download_file(self, url, filepath):\n",
    "        \"\"\"Download file with progress tracking\"\"\"\n",
    "        print(f\"üì• Downloading: {os.path.basename(filepath)}\")\n",
    "        response = self.session.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        downloaded_size = 0\n",
    "        \n",
    "        with open(filepath, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "                    downloaded_size += len(chunk)\n",
    "                    \n",
    "                    if total_size > 0:\n",
    "                        percent = (downloaded_size / total_size) * 100\n",
    "                        print(f\"\\rüì• Progress: {percent:.1f}%\", end=\"\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Download completed\")\n",
    "        return filepath\n",
    "\n",
    "    def read_excel_file(self, file_path):\n",
    "        \"\"\"Read Excel file with multiple engine attempts\"\"\"\n",
    "        engines_to_try = ['openpyxl', 'xlrd']\n",
    "        \n",
    "        for engine in engines_to_try:\n",
    "            try:\n",
    "                data = pd.read_excel(file_path, engine=engine)\n",
    "                return data\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        try:\n",
    "            data = pd.read_excel(file_path)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def download_gtfs_data(self):\n",
    "        \"\"\"Download and extract GTFS data\"\"\"\n",
    "        print(\"üó∫Ô∏è Downloading GTFS Data...\")\n",
    "        \n",
    "        package_info = self.fetch_package(self.gtfs_package_id)\n",
    "        print(f\"üì¶ Package: {package_info['title']}\")\n",
    "        \n",
    "        # Find the Complete GTFS resource\n",
    "        gtfs_resource = None\n",
    "        for resource in package_info['resources']:\n",
    "            if ('complete gtfs' in resource.get('name', '').lower() or \n",
    "                'completegtfs' in resource.get('name', '').lower()):\n",
    "                gtfs_resource = resource\n",
    "                break\n",
    "        \n",
    "        if not gtfs_resource:\n",
    "            raise Exception(\"Complete GTFS resource not found\")\n",
    "        \n",
    "        print(f\"üì• Downloading GTFS ZIP from: {gtfs_resource['url']}\")\n",
    "        \n",
    "        # Download GTFS ZIP\n",
    "        zip_path = os.path.join(self.input_data_folder, \"complete_gtfs.zip\")\n",
    "        self.download_file(gtfs_resource['url'], zip_path)\n",
    "        \n",
    "        # Extract GTFS files\n",
    "        print(\"üîß Extracting GTFS files...\")\n",
    "        gtfs_data = self.extract_gtfs_files(zip_path)\n",
    "        \n",
    "        return gtfs_data\n",
    "\n",
    "    def extract_gtfs_files(self, zip_path):\n",
    "        \"\"\"Extract required files from GTFS ZIP\"\"\"\n",
    "        gtfs_data = {}\n",
    "        required_files = ['routes.txt', 'trips.txt', 'shapes.txt', 'stops.txt']\n",
    "        \n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                file_list = zip_ref.namelist()\n",
    "                print(f\"üìÅ Files in GTFS ZIP: {len(file_list)}\")\n",
    "                \n",
    "                for filename in required_files:\n",
    "                    if filename in file_list:\n",
    "                        with zip_ref.open(filename) as file:\n",
    "                            content = file.read().decode('utf-8')\n",
    "                            gtfs_data[filename] = content\n",
    "                            \n",
    "                        file_path = os.path.join(self.input_data_folder, filename)\n",
    "                        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                            f.write(content)\n",
    "                        \n",
    "                        print(f\"‚úÖ Extracted: {filename}\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è Missing: {filename}\")\n",
    "            \n",
    "            return gtfs_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting GTFS files: {e}\")\n",
    "            raise\n",
    "\n",
    "    def safe_min_max(self, series):\n",
    "        \"\"\"Safely get min and max values for a series, handling mixed data types\"\"\"\n",
    "        try:\n",
    "            numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "            if not numeric_series.isna().all():\n",
    "                valid_values = numeric_series.dropna()\n",
    "                if len(valid_values) > 0:\n",
    "                    return f\"{valid_values.min()} to {valid_values.max()}\"\n",
    "            \n",
    "            datetime_series = pd.to_datetime(series, errors='coerce')\n",
    "            if not datetime_series.isna().all():\n",
    "                valid_dates = datetime_series.dropna()\n",
    "                if len(valid_dates) > 0:\n",
    "                    return f\"{valid_dates.min()} to {valid_dates.max()}\"\n",
    "            \n",
    "            unique_count = series.nunique()\n",
    "            sample_values = series.dropna().unique()[:3]\n",
    "            return f\"[Object type - {unique_count} unique values] Sample: {sample_values}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"[Error: {str(e)}]\"\n",
    "\n",
    "    def clean_delay_data(self, delay_data):\n",
    "        \"\"\"Clean and convert delay data types with detailed debugging\"\"\"\n",
    "        print(\"üßπ Cleaning delay data types...\")\n",
    "        \n",
    "        df = pd.DataFrame(delay_data)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìä FULL DELAY DATA DATAFRAME INFO:\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"üìà DataFrame shape: {df.shape}\")\n",
    "        print(f\"üìã Columns: {list(df.columns)}\")\n",
    "        \n",
    "        print(\"\\nüîç Column details:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"   - {col}: {df[col].dtype}, {df[col].notna().sum()} non-null values\")\n",
    "            if df[col].dtype == 'object':\n",
    "                sample_values = df[col].dropna().unique()[:3]\n",
    "                print(f\"     Sample values: {sample_values}\")\n",
    "        \n",
    "        date_columns = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "        print(f\"\\nüìÖ Date-related columns: {date_columns}\")\n",
    "        \n",
    "        for date_col in date_columns:\n",
    "            if date_col in df.columns:\n",
    "                range_info = self.safe_min_max(df[date_col])\n",
    "                print(f\"   - {date_col}: {range_info}\")\n",
    "        \n",
    "        route_columns = [col for col in df.columns if 'route' in col.lower() or 'line' in col.lower()]\n",
    "        print(f\"\\nüöç Route/Line columns: {route_columns}\")\n",
    "        \n",
    "        for route_col in route_columns:\n",
    "            if route_col in df.columns:\n",
    "                print(f\"   - {route_col}: {df[route_col].nunique()} unique values\")\n",
    "        \n",
    "        delay_columns = [col for col in df.columns if 'delay' in col.lower()]\n",
    "        print(f\"\\n‚è±Ô∏è Delay columns: {delay_columns}\")\n",
    "        \n",
    "        for delay_col in delay_columns:\n",
    "            if delay_col in df.columns:\n",
    "                range_info = self.safe_min_max(df[delay_col])\n",
    "                print(f\"   - {delay_col}: {range_info}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üîß STARTING DATA CLEANING...\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Convert numeric columns\n",
    "        if 'Min Delay' in df.columns:\n",
    "            df['Min Delay'] = pd.to_numeric(df['Min Delay'], errors='coerce').fillna(0)\n",
    "            print(f\"‚úÖ Converted Min Delay to numeric: {len(df[df['Min Delay'] > 0])} valid delays\")\n",
    "        \n",
    "        for delay_col in ['Delay', 'Delay Minutes', 'Delay_Minutes']:\n",
    "            if delay_col in df.columns and 'Min Delay' not in df.columns:\n",
    "                df['Min Delay'] = pd.to_numeric(df[delay_col], errors='coerce').fillna(0)\n",
    "                print(f\"‚úÖ Using '{delay_col}' as Min Delay: {len(df[df['Min Delay'] > 0])} valid delays\")\n",
    "                break\n",
    "        \n",
    "        if 'Min Gap' in df.columns:\n",
    "            df['Min Gap'] = pd.to_numeric(df['Min Gap'], errors='coerce').fillna(0)\n",
    "        \n",
    "        if 'Vehicle' in df.columns:\n",
    "            df['Vehicle'] = pd.to_numeric(df['Vehicle'], errors='coerce').fillna(0)\n",
    "        \n",
    "        date_column_used = None\n",
    "        for date_col in ['Date', 'Incident Date', 'Report Date', 'Date & Time']:\n",
    "            if date_col in df.columns:\n",
    "                df['Date'] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "                date_column_used = date_col\n",
    "                print(f\"‚úÖ Using '{date_col}' as Date column\")\n",
    "                break\n",
    "        \n",
    "        if date_column_used:\n",
    "            date_range_info = self.safe_min_max(df['Date'])\n",
    "            print(f\"üìÖ Date range after cleaning: {date_range_info}\")\n",
    "            if df['Date'].notna().any():\n",
    "                years = df['Date'].dt.year.dropna().unique()\n",
    "                print(f\"üìÖ Years in data: {sorted(years)}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No date column found\")\n",
    "            df['Date'] = pd.to_datetime('2023-01-01')\n",
    "        \n",
    "        if 'Line' in df.columns:\n",
    "            df['Route'] = df['Line']\n",
    "            print(f\"‚úÖ Using 'Line' column as Route: {df['Route'].nunique()} unique routes\")\n",
    "        elif 'Route' not in df.columns:\n",
    "            for route_col in ['Route Number', 'Route No', 'Route_ID']:\n",
    "                if route_col in df.columns:\n",
    "                    df['Route'] = df[route_col]\n",
    "                    print(f\"‚úÖ Using '{route_col}' as Route: {df['Route'].nunique()} unique routes\")\n",
    "                    break\n",
    "        \n",
    "        if 'Station' in df.columns:\n",
    "            df['Location'] = df['Station']\n",
    "            print(f\"‚úÖ Using 'Station' column as Location: {df['Location'].nunique()} unique locations\")\n",
    "        elif 'Location' not in df.columns:\n",
    "            for loc_col in ['Stop', 'Stop Name', 'Station Name', 'Location Name']:\n",
    "                if loc_col in df.columns:\n",
    "                    df['Location'] = df[loc_col]\n",
    "                    print(f\"‚úÖ Using '{loc_col}' as Location: {df['Location'].nunique()} unique locations\")\n",
    "                    break\n",
    "        \n",
    "        if 'Route' in df.columns:\n",
    "            df['Route'] = df['Route'].astype(str)\n",
    "            df['Route_Number'] = df['Route'].str.extract(r'^(\\d+)')\n",
    "            df['Route'] = df['Route_Number'].fillna(df['Route'])\n",
    "            print(f\"‚úÖ Extracted route numbers: {df['Route'].nunique()} unique routes\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚úÖ CLEANED DATAFRAME SUMMARY:\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"üìà Final shape: {df.shape}\")\n",
    "        print(f\"üìã Final columns: {list(df.columns)}\")\n",
    "        \n",
    "        if 'Date' in df.columns:\n",
    "            date_range_info = self.safe_min_max(df['Date'])\n",
    "            print(f\"üìÖ Final date range: {date_range_info}\")\n",
    "            if df['Date'].notna().any():\n",
    "                year_counts = df['Date'].dt.year.value_counts().sort_index()\n",
    "                print(f\"üìÖ Records by year:\\n{year_counts}\")\n",
    "        \n",
    "        if 'Route' in df.columns:\n",
    "            print(f\"üöç Final unique routes: {df['Route'].nunique()}\")\n",
    "        \n",
    "        if 'Min Delay' in df.columns:\n",
    "            valid_delays = len(df[df['Min Delay'] > 0])\n",
    "            print(f\"‚è±Ô∏è Valid delays (>0 min): {valid_delays}/{len(df)} ({valid_delays/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def process_route_performance(self, delay_data):\n",
    "        \"\"\"Process delay data into route performance metrics\"\"\"\n",
    "        print(\"üìà Processing route performance data...\")\n",
    "        \n",
    "        # Clean and convert data types\n",
    "        df = self.clean_delay_data(delay_data)\n",
    "        \n",
    "        # Check if we have route data\n",
    "        if 'Route' not in df.columns:\n",
    "            print(\"‚ùå No 'Route' column found in delay data\")\n",
    "            for col in df.columns:\n",
    "                if 'route' in col.lower() or 'line' in col.lower():\n",
    "                    df['Route'] = df[col]\n",
    "                    print(f\"‚úÖ Using '{col}' as Route column\")\n",
    "                    break\n",
    "        \n",
    "        if 'Route' not in df.columns:\n",
    "            print(\"‚ùå No route data available\")\n",
    "            return []\n",
    "        \n",
    "        # Ensure Route column is string\n",
    "        df['Route'] = df['Route'].astype(str)\n",
    "        \n",
    "        # Filter out routes with no valid delays\n",
    "        df_valid = df[df['Min Delay'] > 0]\n",
    "        \n",
    "        if len(df_valid) == 0:\n",
    "            print(\"‚ö†Ô∏è No valid delays found\")\n",
    "            return []\n",
    "        \n",
    "        # Group by route and calculate metrics\n",
    "        route_groups = df_valid.groupby('Route').agg({\n",
    "            'Min Delay': ['count', 'mean', 'sum'],\n",
    "            'Vehicle': 'nunique'\n",
    "        }).round(2)\n",
    "        \n",
    "        # Flatten column names\n",
    "        route_groups.columns = ['Delay_Count', 'Avg_Delay_Min', 'Total_Delay_Min', 'Unique_Vehicles']\n",
    "        route_groups = route_groups.reset_index()\n",
    "        \n",
    "        # NEW: Apply filters - only routes with more than 10 delays and exclude routes 1-4\n",
    "        print(\"üîç Applying filters: routes with >10 delays and excluding routes 1-4\")\n",
    "        route_groups = route_groups[\n",
    "            (route_groups['Delay_Count'] > 10) & \n",
    "            (~route_groups['Route'].isin(['1', '2', '3', '4']))\n",
    "        ]\n",
    "        \n",
    "        print(f\"üìä After filtering: {len(route_groups)} routes remaining\")\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        total_days = df['Date'].nunique() if 'Date' in df.columns and df['Date'].notna().any() else 30\n",
    "        route_groups['Delays_Per_Day'] = (route_groups['Delay_Count'] / total_days).round(2)\n",
    "        route_groups['On_Time_Percentage'] = 0  # Would need schedule data\n",
    "        \n",
    "        # Add route names\n",
    "        route_groups['route_long_name'] = route_groups['Route'].apply(lambda x: f\"Route {x}\")\n",
    "        \n",
    "        # Convert to list of dictionaries\n",
    "        route_performance = route_groups.to_dict('records')\n",
    "        \n",
    "        print(f\"‚úÖ Processed {len(route_performance)} routes (filtered: >10 delays, excluding 1-4)\")\n",
    "        return route_performance\n",
    "\n",
    "    def process_route_geometries(self, gtfs_data):\n",
    "        \"\"\"Process GTFS data into route geometries\"\"\"\n",
    "        print(\"üó∫Ô∏è Processing route geometries...\")\n",
    "        \n",
    "        route_geometries = {}\n",
    "        \n",
    "        try:\n",
    "            if 'shapes.txt' in gtfs_data:\n",
    "                shapes_path = os.path.join(self.input_data_folder, 'shapes.txt')\n",
    "                trips_path = os.path.join(self.input_data_folder, 'trips.txt')\n",
    "                routes_path = os.path.join(self.input_data_folder, 'routes.txt')\n",
    "                \n",
    "                if (os.path.exists(shapes_path) and \n",
    "                    os.path.exists(trips_path) and \n",
    "                    os.path.exists(routes_path)):\n",
    "                    \n",
    "                    shapes_df = pd.read_csv(shapes_path, dtype={'shape_id': str})\n",
    "                    trips_df = pd.read_csv(trips_path, dtype={'route_id': str, 'shape_id': str})\n",
    "                    routes_df = pd.read_csv(routes_path, dtype={'route_id': str})\n",
    "                    \n",
    "                    print(f\"üìä Shapes: {len(shapes_df)}, Trips: {len(trips_df)}, Routes: {len(routes_df)}\")\n",
    "                    \n",
    "                    shapes_by_route = {}\n",
    "                    for shape_id, group in shapes_df.groupby('shape_id'):\n",
    "                        coords = group.sort_values('shape_pt_sequence')[['shape_pt_lat', 'shape_pt_lon']].values.tolist()\n",
    "                        shapes_by_route[shape_id] = coords\n",
    "                    \n",
    "                    route_to_shape = {}\n",
    "                    for _, trip in trips_df.iterrows():\n",
    "                        if pd.notna(trip['route_id']) and pd.notna(trip['shape_id']):\n",
    "                            route_to_shape[trip['route_id']] = trip['shape_id']\n",
    "                    \n",
    "                    for route_id, shape_id in route_to_shape.items():\n",
    "                        if shape_id in shapes_by_route:\n",
    "                            coordinates = []\n",
    "                            for lat, lon in shapes_by_route[shape_id]:\n",
    "                                if (isinstance(lat, (int, float)) and isinstance(lon, (int, float)) and\n",
    "                                    -90 <= lat <= 90 and -180 <= lon <= 180):\n",
    "                                    coordinates.append([float(lat), float(lon)])\n",
    "                            \n",
    "                            if coordinates:\n",
    "                                route_geometries[str(route_id)] = coordinates\n",
    "                    \n",
    "                    print(f\"‚úÖ Processed {len(route_geometries)} route geometries from GTFS\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è GTFS files not found, generating sample geometries\")\n",
    "                    self.create_sample_geometries(route_geometries)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No shapes.txt found, generating sample geometries\")\n",
    "                self.create_sample_geometries(route_geometries)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing GTFS geometries: {e}\")\n",
    "            print(\"üîÑ Generating sample geometries instead\")\n",
    "            self.create_sample_geometries(route_geometries)\n",
    "        \n",
    "        return route_geometries\n",
    "\n",
    "    def create_sample_geometries(self, route_geometries):\n",
    "        \"\"\"Create sample geometries when GTFS data is not available\"\"\"\n",
    "        toronto_center = [43.6532, -79.3832]\n",
    "        routes = ['501', '504', '505', '506', '509', '510', '511', '512', '96', '165', '102', '35']\n",
    "        \n",
    "        for i, route in enumerate(routes):\n",
    "            coordinates = []\n",
    "            point_count = 8 + i\n",
    "            \n",
    "            for j in range(point_count):\n",
    "                angle = (j / point_count) * 3.14\n",
    "                lat = toronto_center[0] + (0.01 * i) + (0.005 * math.cos(angle))\n",
    "                lng = toronto_center[1] + (0.01 * i) + (0.005 * math.sin(angle))\n",
    "                coordinates.append([round(lat, 6), round(lng, 6)])\n",
    "            \n",
    "            route_geometries[route] = coordinates\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(route_geometries)} sample route geometries\")\n",
    "\n",
    "    def process_location_analysis(self, delay_data):\n",
    "        \"\"\"Process delay data into location analysis\"\"\"\n",
    "        print(\"üìç Processing location analysis...\")\n",
    "        \n",
    "        df = self.clean_delay_data(delay_data)\n",
    "        \n",
    "        if 'Location' not in df.columns:\n",
    "            print(\"‚ùå No 'Location' column found in delay data\")\n",
    "            for col in df.columns:\n",
    "                if 'location' in col.lower() or 'station' in col.lower() or 'stop' in col.lower():\n",
    "                    df['Location'] = df[col]\n",
    "                    print(f\"‚úÖ Using '{col}' as Location column\")\n",
    "                    break\n",
    "        \n",
    "        if 'Location' not in df.columns:\n",
    "            print(\"‚ùå No location data available\")\n",
    "            return []\n",
    "        \n",
    "        df_with_location = df[df['Location'].notna() & (df['Location'] != '') & (df['Location'] != 'Unknown')]\n",
    "        \n",
    "        if len(df_with_location) == 0:\n",
    "            print(\"‚ö†Ô∏è No location data found\")\n",
    "            return []\n",
    "        \n",
    "        df_valid = df_with_location[df_with_location['Min Delay'] > 0]\n",
    "        \n",
    "        if len(df_valid) == 0:\n",
    "            print(\"‚ö†Ô∏è No valid delays at locations found\")\n",
    "            return []\n",
    "        \n",
    "        location_groups = df_valid.groupby('Location').agg({\n",
    "            'Min Delay': ['count', 'mean'],\n",
    "            'Route': 'nunique',\n",
    "            'Vehicle': 'nunique'\n",
    "        }).round(2)\n",
    "        \n",
    "        location_groups.columns = ['total_delays', 'avg_delay_min', 'route_count', 'vehicle_count']\n",
    "        location_groups = location_groups.reset_index()\n",
    "        \n",
    "        location_analysis = []\n",
    "        for _, row in location_groups.iterrows():\n",
    "            location_analysis.append({\n",
    "                'location_id': self.sanitize_location_id(row['Location']),\n",
    "                'location_name': row['Location'],\n",
    "                'total_delays': int(row['total_delays']),\n",
    "                'avg_delay_min': float(row['avg_delay_min']),\n",
    "                'latitude': self.generate_toronto_lat(),\n",
    "                'longitude': self.generate_toronto_lng(),\n",
    "                'route_count': int(row['route_count']),\n",
    "                'vehicle_count': int(row['vehicle_count']),\n",
    "                'peak_hours': json.dumps(['07:00-09:00', '16:00-18:00'])\n",
    "            })\n",
    "        \n",
    "        location_analysis.sort(key=lambda x: x['total_delays'], reverse=True)\n",
    "        \n",
    "        print(f\"‚úÖ Processed {len(location_analysis)} locations\")\n",
    "        return location_analysis\n",
    "\n",
    "    def sanitize_location_id(self, location_name):\n",
    "        \"\"\"Create a sanitized location ID\"\"\"\n",
    "        return (location_name.lower()\n",
    "                .replace(' ', '_')\n",
    "                .replace('/', '_')\n",
    "                .replace('\\\\', '_')\n",
    "                .replace('&', 'and')\n",
    "                .replace(\"'\", '')\n",
    "                .replace('\"', '')\n",
    "                .replace('(', '')\n",
    "                .replace(')', '')\n",
    "                .replace(',', '')[:50])\n",
    "\n",
    "    def generate_toronto_lat(self):\n",
    "        \"\"\"Generate random Toronto latitude\"\"\"\n",
    "        return round(43.65 + (random.random() - 0.5) * 0.1, 6)\n",
    "\n",
    "    def generate_toronto_lng(self):\n",
    "        \"\"\"Generate random Toronto longitude\"\"\"\n",
    "        return round(-79.38 + (random.random() - 0.5) * 0.1, 6)\n",
    "\n",
    "    def process_summary_statistics(self, delay_data, route_performance, location_analysis):\n",
    "        \"\"\"Calculate summary statistics\"\"\"\n",
    "        print(\"üìä Processing summary statistics...\")\n",
    "        \n",
    "        df = self.clean_delay_data(delay_data)\n",
    "        \n",
    "        total_delays = len(delay_data)\n",
    "        \n",
    "        valid_delays = len(df[df['Min Delay'] > 0])\n",
    "        avg_delay = df[df['Min Delay'] > 0]['Min Delay'].mean() if valid_delays > 0 else 0\n",
    "        \n",
    "        unique_routes = df['Route'].nunique() if 'Route' in df.columns else 0\n",
    "        unique_vehicles = df['Vehicle'].nunique() if 'Vehicle' in df.columns else 0\n",
    "        unique_locations = df['Location'].nunique() if 'Location' in df.columns else 0\n",
    "        \n",
    "        oldest_date = None\n",
    "        most_recent_date = None\n",
    "        if 'Date' in df.columns and df['Date'].notna().any():\n",
    "            oldest_date = df['Date'].min()\n",
    "            most_recent_date = df['Date'].max()\n",
    "            print(f\"üìÖ Date range found: {oldest_date} to {most_recent_date}\")\n",
    "        \n",
    "        total_routes_in_data = df['Route'].nunique() if 'Route' in df.columns else 0\n",
    "        displayed_routes = len(route_performance)\n",
    "        \n",
    "        if total_routes_in_data > 0:\n",
    "            coverage_percentage = round((displayed_routes / total_routes_in_data) * 100, 1)\n",
    "        else:\n",
    "            coverage_percentage = 0\n",
    "        \n",
    "        print(f\"üìà Coverage calculation: {displayed_routes} displayed / {total_routes_in_data} total = {coverage_percentage}%\")\n",
    "        \n",
    "        most_delayed_route = None\n",
    "        if route_performance:\n",
    "            most_delayed_route = max(route_performance, key=lambda x: x['Avg_Delay_Min'])\n",
    "        \n",
    "        data_period = \"Unknown\"\n",
    "        if oldest_date and most_recent_date:\n",
    "            oldest_year = oldest_date.year\n",
    "            most_recent_year = most_recent_date.year\n",
    "            if oldest_year == most_recent_year:\n",
    "                data_period = str(most_recent_year)\n",
    "            else:\n",
    "                data_period = f\"{oldest_year}-{most_recent_year}\"\n",
    "        \n",
    "        stats = {\n",
    "            'total_delays': total_delays,\n",
    "            'valid_delays': valid_delays,\n",
    "            'avg_delay_minutes': round(avg_delay, 2),\n",
    "            'unique_routes': unique_routes,\n",
    "            'unique_vehicles': unique_vehicles,\n",
    "            'unique_locations': unique_locations,\n",
    "            'data_points': total_delays,\n",
    "            'coverage_percentage': coverage_percentage,\n",
    "            'time_period': data_period,\n",
    "            'updated_at': datetime.now().isoformat(),\n",
    "            'data_refresh_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'data_oldest_date': oldest_date.isoformat() if oldest_date else None,\n",
    "            'data_most_recent_date': most_recent_date.isoformat() if most_recent_date else None,\n",
    "            'data_update_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'peak_delay_hour': self.calculate_peak_hour(df),\n",
    "            'most_delayed_route': f\"{most_delayed_route['Route']} - {most_delayed_route['route_long_name']}\" if most_delayed_route else 'Unknown',\n",
    "            'displayed_routes_count': displayed_routes,\n",
    "            'total_routes_count': total_routes_in_data,\n",
    "            'data_quality': {\n",
    "                'valid_delay_percentage': round((valid_delays / total_delays * 100), 2) if total_delays > 0 else 0,\n",
    "                'route_coverage': unique_routes,\n",
    "                'location_coverage': unique_locations,\n",
    "                'date_range_available': oldest_date is not None and most_recent_date is not None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Summary statistics calculated\")\n",
    "        print(f\"   - Data Period: {data_period}\")\n",
    "        print(f\"   - Coverage: {coverage_percentage}% ({displayed_routes}/{total_routes_in_data} routes)\")\n",
    "        print(f\"   - Date Range: {oldest_date} to {most_recent_date}\" if oldest_date else \"   - No date range available\")\n",
    "        \n",
    "        return stats\n",
    "\n",
    "    def calculate_peak_hour(self, df):\n",
    "        \"\"\"Calculate peak delay hour from data\"\"\"\n",
    "        try:\n",
    "            if 'Time' in df.columns:\n",
    "                time_series = df['Time'].dropna()\n",
    "                if len(time_series) > 0:\n",
    "                    time_strings = time_series.astype(str)\n",
    "                    hours = []\n",
    "                    for time_str in time_strings:\n",
    "                        try:\n",
    "                            if ':' in time_str:\n",
    "                                hour_part = time_str.split(':')[0]\n",
    "                                hour = int(hour_part)\n",
    "                                hours.append(hour)\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    if hours:\n",
    "                        hour_series = pd.Series(hours)\n",
    "                        peak_hour = int(hour_series.mode().iloc[0]) if not hour_series.mode().empty else 8\n",
    "                        return f\"{peak_hour:02d}:00\"\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error calculating peak hour: {e}\")\n",
    "        \n",
    "        return \"08:00\"\n",
    "\n",
    "    def save_processed_data(self, route_performance, route_geometries, location_analysis, summary_stats):\n",
    "        \"\"\"Save all processed data to output folder\"\"\"\n",
    "        print(\"üíæ Saving processed data...\")\n",
    "        \n",
    "        route_performance_path = os.path.join(self.output_data_folder, \"route_performance.csv\")\n",
    "        with open(route_performance_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            if route_performance:\n",
    "                writer = csv.DictWriter(f, fieldnames=route_performance[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(route_performance)\n",
    "        print(f\"‚úÖ Saved route_performance.csv ({len(route_performance)} routes)\")\n",
    "        \n",
    "        route_geometries_path = os.path.join(self.output_data_folder, \"route_geometries.json\")\n",
    "        with open(route_geometries_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(route_geometries, f, indent=2)\n",
    "        print(f\"‚úÖ Saved route_geometries.json ({len(route_geometries)} routes)\")\n",
    "        \n",
    "        location_analysis_path = os.path.join(self.output_data_folder, \"location_analysis.csv\")\n",
    "        with open(location_analysis_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            if location_analysis:\n",
    "                writer = csv.DictWriter(f, fieldnames=location_analysis[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(location_analysis)\n",
    "        print(f\"‚úÖ Saved location_analysis.csv ({len(location_analysis)} locations)\")\n",
    "        \n",
    "        summary_stats_path = os.path.join(self.output_data_folder, \"summary_statistics.json\")\n",
    "        with open(summary_stats_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary_stats, f, indent=2, default=str)\n",
    "        print(\"‚úÖ Saved summary_statistics.json\")\n",
    "\n",
    "    def should_update_data(self):\n",
    "        \"\"\"Check if data needs to be updated (older than 1 hour)\"\"\"\n",
    "        stats_file = os.path.join(self.output_data_folder, \"summary_statistics.json\")\n",
    "        \n",
    "        if not os.path.exists(stats_file):\n",
    "            return True\n",
    "        \n",
    "        try:\n",
    "            with open(stats_file, 'r', encoding='utf-8') as f:\n",
    "                stats = json.load(f)\n",
    "            \n",
    "            if 'updated_at' in stats:\n",
    "                last_updated = datetime.fromisoformat(stats['updated_at'].replace('Z', '+00:00'))\n",
    "                one_hour_ago = datetime.now() - timedelta(hours=1)\n",
    "                return last_updated < one_hour_ago\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def transform_data(self):\n",
    "        \"\"\"Main transformation function\"\"\"\n",
    "        print(\"üîÑ Starting TTC Data Transformation...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Download raw data using NEW method\n",
    "            print(\"\\nüì• Downloading raw data using new method...\")\n",
    "            delay_data = self.download_delay_data()\n",
    "            gtfs_data = self.download_gtfs_data()\n",
    "            \n",
    "            print(\"\\n‚úÖ Raw data downloaded successfully\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            # Step 2: Process data\n",
    "            print(\"\\nüîß Processing data...\")\n",
    "            route_performance = self.process_route_performance(delay_data)\n",
    "            route_geometries = self.process_route_geometries(gtfs_data)\n",
    "            location_analysis = self.process_location_analysis(delay_data)\n",
    "            summary_stats = self.process_summary_statistics(delay_data, route_performance, location_analysis)\n",
    "            \n",
    "            print(\"\\n‚úÖ Data processing completed\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            # Step 3: Save processed data\n",
    "            print(\"\\nüíæ Saving processed data...\")\n",
    "            self.save_processed_data(route_performance, route_geometries, location_analysis, summary_stats)\n",
    "            \n",
    "            print(\"\\nüéâ Transformation completed successfully!\")\n",
    "            print(\"=\" * 50)\n",
    "            print(\"üìä Summary:\")\n",
    "            print(f\"   - Routes: {len(route_performance)} (filtered: >10 delays, excluding 1-4)\")\n",
    "            print(f\"   - Geometries: {len(route_geometries)}\")\n",
    "            print(f\"   - Locations: {len(location_analysis)}\")\n",
    "            print(f\"   - Total Delays: {summary_stats['total_delays']}\")\n",
    "            print(f\"   - Valid Delays: {summary_stats['valid_delays']}\")\n",
    "            print(f\"   - Average Delay: {summary_stats['avg_delay_minutes']} minutes\")\n",
    "            print(f\"   - Coverage: {summary_stats['coverage_percentage']}%\")\n",
    "            print(f\"   - Data Period: {summary_stats['time_period']}\")\n",
    "            print(f\"   - Date Range: {summary_stats.get('data_oldest_date', 'N/A')} to {summary_stats.get('data_most_recent_date', 'N/A')}\")\n",
    "            print(f\"\\nüìÅ Output folder: {self.output_data_folder}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nüí• Transformation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transformer = TTCDataTransformer()\n",
    "    success = transformer.transform_data()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n‚ú® Data update completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Data update failed!\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "832a5d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map updated with 1096 buses at 2025-11-15 05:32:38.151508\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import folium\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class TTCBusTracker:\n",
    "    def __init__(self):\n",
    "        self.bus_data = []\n",
    "        \n",
    "    def fetch_bus_data(self):\n",
    "        \"\"\"Fetch current bus positions\"\"\"\n",
    "        url = \"https://bustime.ttc.ca/gtfsrt/vehicles\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                feed = gtfs_realtime_pb2.FeedMessage()\n",
    "                feed.ParseFromString(response.content)\n",
    "                \n",
    "                buses = []\n",
    "                for entity in feed.entity:\n",
    "                    if entity.HasField('vehicle'):\n",
    "                        vehicle = entity.vehicle\n",
    "                        if vehicle.position.latitude != 0 and vehicle.position.longitude != 0:\n",
    "                            buses.append({\n",
    "                                'id': vehicle.vehicle.id,\n",
    "                                'route': vehicle.trip.route_id,\n",
    "                                'lat': vehicle.position.latitude,\n",
    "                                'lon': vehicle.position.longitude,\n",
    "                                'timestamp': datetime.fromtimestamp(vehicle.timestamp)\n",
    "                            })\n",
    "                self.bus_data = buses\n",
    "                return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def create_live_map(self):\n",
    "        \"\"\"Create an auto-refreshing map\"\"\"\n",
    "        if not self.bus_data:\n",
    "            print(\"No bus data available\")\n",
    "            return\n",
    "        \n",
    "        # Center map on average of bus positions\n",
    "        avg_lat = sum(bus['lat'] for bus in self.bus_data) / len(self.bus_data)\n",
    "        avg_lon = sum(bus['lon'] for bus in self.bus_data) / len(self.bus_data)\n",
    "        \n",
    "        m = folium.Map(location=[avg_lat, avg_lon], zoom_start=12)\n",
    "        \n",
    "        for bus in self.bus_data:\n",
    "            folium.CircleMarker(\n",
    "                [bus['lat'], bus['lon']],\n",
    "                radius=8,\n",
    "                popup=f\"Bus {bus['id']}<br>Route {bus['route']}\",\n",
    "                tooltip=f\"Route {bus['route']}\",\n",
    "                color='blue',\n",
    "                fill=True\n",
    "            ).add_to(m)\n",
    "        \n",
    "        # Add update time\n",
    "        folium.Marker(\n",
    "            [avg_lat, avg_lon],\n",
    "            icon=folium.DivIcon(\n",
    "                html=f'<div style=\"font-size: 16pt\">Last updated: {datetime.now().strftime(\"%H:%M:%S\")}</div>'\n",
    "            )\n",
    "        ).add_to(m)\n",
    "        \n",
    "        m.save('ttc_live_map.html')\n",
    "        print(f\"Map updated with {len(self.bus_data)} buses at {datetime.now()}\")\n",
    "\n",
    "# Usage\n",
    "tracker = TTCBusTracker()\n",
    "if tracker.fetch_bus_data():\n",
    "    tracker.create_live_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbc61b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gtfs-realtime-bindings\n",
      "  Downloading gtfs-realtime-bindings-1.0.0.tar.gz (6.2 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from gtfs-realtime-bindings) (80.9.0)\n",
      "Collecting protobuf (from gtfs-realtime-bindings)\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Building wheels for collected packages: gtfs-realtime-bindings\n",
      "  Building wheel for gtfs-realtime-bindings (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gtfs-realtime-bindings: filename=gtfs_realtime_bindings-1.0.0-py3-none-any.whl size=6027 sha256=5187a5440abf782c35476741fbb5848d906fe668b0f844c587b8eb8cec37bf0e\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/b6/43/38/17a10a2cdd30cb86acceb42e24e7d2d6bb98b2c59ff8983e20\n",
      "Successfully built gtfs-realtime-bindings\n",
      "Installing collected packages: protobuf, gtfs-realtime-bindings\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [gtfs-realtime-bindings]\n",
      "\u001b[1A\u001b[2KSuccessfully installed gtfs-realtime-bindings-1.0.0 protobuf-6.33.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gtfs-realtime-bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea1ad65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
